第一章 Linux内核简介
1.3 操作系统内核简介
<1>操作系统是指在整个系统中负责完成最 基本功能和系统管理 的那些部分。这些部分应包括内核、设备驱动程序、启动引导程序、
      命令行Shell或者其他种类的用户界面、基本文件管理工具和系统工具。

<2>系统这个词包含了操作系统和所有运行在其上的应用程序。
	  
<3>用户界面是操作系统的外在表象，内核才是操作系统的内在核心。系统其他部分必须依靠内核这部分软件提供的服务，
像管理硬件设备、分配系统资源等。内核有时候被称为管理者或操作系统核心。
	通常一个内核由负责响应中断的中断服务程序；负责管理多个进程从而分享处理器时间的调度程序；
	负责管理进程地址空间的内存管理程序；网络、进程间通信等系统服务程序共同组成。
	  
<4>内核独立于普通应用程序，它一般处于系统态，拥有受保护的内存空间和访问硬件设备的所有权限。这种系统态和受保护的内存空间，统称为内核空间。

<5>应用程序在用户空间执行，它们只能看到允许它们使用的部分系统资源，并且只使用某些特定的系统功能，不能直接访问硬件，
	  也不能访问内核划分给别人的内存范围，还有其他一些使用限制。
	  
<6>当内核运行时，系统以内核态进入内核空间执行；执行普通程序时，系统以用户态进入用户空间执行。

<7>应用程序-->系统调用接口-->内核子系统-->设备驱动程序-->硬件
						  -->设备驱动程序             -->硬件
1)在系统中运行的应用程序通过系统调用来与内核通信。应用程序通常调用库函数，再由库函数通过系统调用界面，让内核代其完成各种不同任务。
   
2)当一个应用程序执行一条系统调用，称为 内核正在代其执行。
	应用程序被称为通过系统调用在内核空间运行；内核被称为运行于进程上下文中。
    应用程序通过系统调用界面陷入内核，是应用程序完成其工作的基本行为方式。
	
<8>内核还负责管理系统的硬件设备   //中断机制
	当硬件设备想要和系统通信时，它首先要发出一个异步的中断信号去打断处理器的执行，继而打断内核的执行。
	中断通常对应一个中断号，内核通过这个中断号查找相应的中断服务程序，并调用这个程序相应和处理中断。
	
<9>处理器在任何时间点上的活动必然为以下三者之一：
1)运行于用户空间，执行用户程序
2)运行于内核空间，处理进程上下文，代表某个特定的进程执行
3)运行于内核空间，处理中断上下文，与任何进程无关，处理某个特定的中断

//这些上下文代表着内核活动的范围。
当CPU空闲时，内核就运行一个空进程，处于进程上下文，但运行于内核空间。
	
1.4 Linux内核和传统Unix内核的比较
1、单内核与微内核设计之比较
<1>单内核
1)把内核从整体上作为一个独立的大过程来实现，同时也运行在一个单独的地址空间上。
这样的内核通常以单个静态二进制文件的形式存放于磁盘中，所有内核服务都在这样的一个大内核地址空间上运行。

2)简单、高性能，大多数Unix系统都设计为单模块。
	
<2>微内核
微内核的功能被划分为多个独立的过程，每个过程叫做一个服务器。
理想情况下，只有强烈请求特权服务的服务器才运行在特权模式下，其他服务器都运行在用户空间。不过，所有服务器都保持独立并运行在各自地址空间上。

1)不能像单模块那样直接调用函数，而是通过消息传递处理微内核通信，(IPC 进程间通信机制)
2)IPC机制开销大于函数调用;同时涉及内核空间和用户空间的上下文切换。
3)结果，所有实际基于微内核的系统都让大部分或全部服务器位于内核，这样虽可以直接调用函数、消除频繁切换上下文的弊端；
	但违反了微内核设计的初衷。
	
2、linux是一个单内核，也就是说，linux内核运行在单独的内核地址空间上。

3、Linux内核与传统Unix系统之间的差异：
<1>Linux支持动态加载/卸载内核模块。
<2>Linux支持对称多处理(SMP)机制。
<3>Linux内核可以抢占。
<4>Linux对线程支持的实现：内核并不区分线程和其他的一般进程。
<5>Linux提供具有设备类的面向对象的设备模型、热插拔时间。
<6>忽略了一些不好的Unix特性
<7>自由

1.5 Linux内核版本
<1>linux内核分为：稳定和处于开发中 两种。稳定内核具有工业级强度。
<2>linux通过一个简单的命名机制来区分稳定的和处于开发的内核。
2.6.26.1 //主版本号.副版本号.修订版本号.稳定版本号(可选)
1)从副版本号就可以反映出该内核是一个稳定版本还是一个处于开发中的版本：偶数表示稳定版本，奇数表示开发版本。
2)主版本号和副版本号 一起描述了 内核系列。2.6版内核系列

第二章 从内核出发
2.3 编译内核
一、配置内核
<1>make config  //字符界面下的命令行工具
该工具会逐一遍历所有配置项，要求用户选择yes、no或者module。
<2>make menuconfig //基于ncurse库编制的图形界面工具
<3>make gconfig  //基于gtk+的图形工具
<4>make defconfig //基于默认的配置为你的体系结构创建一个配置

执行上述任意一条命令后会生成.config文件保存配置项结果，可以直接修改它。

<5>修改配置文件后，或利用已有的配置文件配置新的代码树时，要验证和更新配置：
make oldconfig

<6>配置选项CONFIG_IKCONFIG_PROC把完整的压缩过的内核配置文件存放在/proc/config.gz下，这样编译一个新内核时就可以很方便地克隆当前的配置。
zcat /proc/config.gz > .config
make oldconfig

<7>一旦内核配置好了就可以编译它了：
make

二、减少编译的垃圾信息
make > .. /detritus

make > /dev/null

三、衍生多个编译作业
<1>make程序能把编译过程拆分成多个并行的作业。其中每个作业独立并发地运行，这有助于极大地加快多处理器系统上的编译过程。
<2>默认情况下，make只衍生一个作业，因为makefile常会出现不正确的依赖信息。多个作业可能互相踩踏，导致编译过程出错。
<3>make -jn //n为要衍生出的作业数目，通常每个处理器上一般衍生出一个或两个作业。
<4>利用distcc或者ccache工具，也可以动态地改善内核的编译时间。

四、完整性内核
<1>怎么安装内核和体系结构以及启动引导工具(boot loader)息息相关。按照启动引导工具的指导将内核镜像拷贝到合适的位置，并且按照启动要求安装。
<2>一定要保证随时有一个或两个可以启动的内核，防止新编译的内核出现问题。


2.4 内核开发的特点
<1>内核编程时既不能访问C库也不能访问标准C头文件
<2>内核编程时必须使用GNU C
<3>内核编程时缺乏像用户空间那样的内存保护机制
<4>内核编程时难以执行浮点运算
<5>内核给每个进程只有一个很小的定长堆栈
<6>由于内核支持异步中断、抢占和SMP，因此必须时刻注意同步和并发
<7>要考虑可移植性的重要性

一、无libc库抑或无标准头文件
<1>对于内核来说，完整的C库或者它的一个子集，都太大、太低效了。
<2>大部分常用的C库函数在内核中都已经得到实现。如linux/string.h

1、头文件

二、GNU C
1、内核并不完全符合ANSI C标准。事实上，内核开发者总是要用到gcc提供的许多语言的扩展部分。(gcc是多种GNU编译器的集合。)
2、内核开发者使用的C语言涵盖了ISO C99标准和GNU C扩展特性。推荐使用gcc 4.4或之后的版本。

3、GNU C与标准C的一些区别
<1>内联函数 inline
C99和GNU C均支持内联函数，可以消除函数调用和返回所带来的开销(寄存器存储和恢复)。
						  编译器会把调用函数的代码和函数本身放在一起优化，所以也有进一步优化代码的可能。
						  代价是代码会变长，占用更多的内存空间和指令缓存。
-内核开发者通常把对时间要求较高，且本身长度较短的函数定义成内联函数。   static inline
-内联函数必须在使用前就定义好，否则编译器就没法把这个函数展开。
-在内核中，为了类型安全和易读性，优先使用内联函数而不是复杂的宏。

<2>内联汇编
-gcc编译器支持在C函数中嵌入汇编指令。通常使用asm()指令嵌入汇编代码。
-linux内核混合使用了C语言和汇编语言。在偏近体系结构的底层或对执行时间要求严格的地方，一般使用汇编语言。内核其他大部分代码使用C语言编写。

<3>分支表明
对于条件选择语句，gcc内建了一条指令用于优化，在一个条件经常出现，或者很少出现的时候，编译器可以根据这条指令对条件分支选择进行优化。
内核把这条指令封装成了宏，如likely()和unlikely()。

三、没有内存保护机制
<1>如果一个用户程序试图进行一次非法的内存访问，内核就会发现这个错误，发送SIGSEGV信号，并结束整个进程。
	然而，如果室内和自己非法访问了内存，那后果就很难控制了。
<2>内核中的内存都不分页。也就是说，每用掉一个字节，物理内存就会减少一个字节。

四、不要轻易在内核中使用浮点数
<1>在用户空间的进程内进行浮点操作时，内核会完成从整数操作到浮点数操作的模式转换。因体系结构不同，内核会执行不同的操作。
<2>与用户空间进程不同，内核并不能完美地支持浮点操作，因为它本身不能陷入。

五、容积小而固定的栈
<1>用户空间的程序可以从栈上分配大量的空间来存放变量，甚至巨大的结构体或者是包含数以千计的数据项的数组。
	这是因为用户空间的栈本身就比较大，而且还能动态地增长。
<2>内核栈的准确大小随体系结构变化。内核栈的大小固定两页，也就是说，32位机的内核栈是8KB；64位机是16KB。
	每个处理机都有自己的栈。
	
六、同步和并发
内核很容易产生竞争条件，内核许多特性都要求能够并发地访问共享数据。

七、可移植性的重要性

第三章、进程管理
3.1 进程
<1>进程：进程就是处于执行期的程序。包括：可执行程序代码、打开的文件、挂起的信号、内核内部数据、处理器状态、内存地址空间、
		执行线程、数据段等。
<2>线程：执行线程简称线程，是在进程中活动的对象。每个线程都拥有一个独立的程序计数器、进程栈和一组进程寄存器。
		 内核调度的对象是线程，而不是进程。
		 对linux而言，线程不过是一种特殊的进程。并不特别区分线程和进程。

<3>虚拟处理器：虽然实际上是多个进程共享一个处理器，但虚拟处理器让进程觉得自己在独享处理器。
<4>虚拟内存：让进程在分配和管理内存时觉得自己拥有整个系统的所有内存资源。
注：同一个进程中  线程之间可以共享虚拟内存，但每个线程都拥有各自的虚拟处理器。

<5>程序本身并不是进程，进程是处于执行期的程序以及相关资源的总称。
	完全可能存在两个或多个不同的进程执行同一个程序，并且两个或两个以上的进程可以共享打开的文件、地址空间之类的资源。
	
<6>fork()系统调用通过复制一个现有进程来创建一个全新的进程。
1)调用fork的进程称为父进程，产生的进程称为子进程。
2)fork系统调用从内核返回两次：一次回到父进程，另一回到新产生的子进程。在返回点这个相同位置上，父进程恢复执行，子进程开始执行。
3)fork()实际上由clone()系统调用实现的。
4)创建新的进程都是为了立即执行新的、不同的程序，而接着调用exec()这组函数可以创建新的地址空间，并把新的程序载入其中。
5)程序通过exit()系统调用退出执行。该系统调用会终止进程并将其占用的资源释放。
	父进程可以通过wait4()系统调用查询子进程是否终结，这其实使得进程拥有等待特定进程执行完毕的能力。
	进程退出执行后被设置为僵死状态，直到它的父进程调用wait()或waitpid()为止。
	
<7>通常创建新的进程都是为了立即执行新的、不同的程序，而接着调用exec()这组函数就可以创建新的地址空间，并把新的程序载入其中。

<8>进程的另一个名字是任务(task)。linux内核通常把进程也叫作任务。这里通常指的是从内核观点所看到的进程。

3.2 进程描述符及任务结构
1、内核把进程列表存放在叫做任务队列(task list)的双向循环链表中。其中每一项都是task_struct类型，称为进程描述符(process descriptor)结构。
	该结构在<linux/sched.h>中，该描述符中包含了一个具体进程的所有信息。
	
一、分配进程描述符
1、linux通过slab分配器分配task_struct结构，这样能达到对象复用和缓存着色(cache coloring)的目的。//进程创建迅速。
<1>通过预先分配和重复使用task_struct，可以避免动态分配和释放所带来的资源消耗。
<2>由于用slab分配器动态生成task_struct，所以只需在栈底(对于向下增加的栈，向低地址增长，栈底为高地址)或
	栈顶(对于向上增加的栈，向高地址增长)创建一个新的结构thread_info。

	向上增长：向高地址增长；向下增长：向低地址增长  //http://blog.csdn.net/pfanaya/article/details/7043992

<3>每个任务的thread_info结构在它的内核栈的尾端部分(低地址部分)。thread_info中的task域中存放着指向该任务实际task_struct的指针。

二、进程描述符的存放
1、PID标识每一个进程，pid_t类型(实际上是int型)
<1>为了与老版本兼容，最大默认设置为32768(short int的最大值)。可以通过/proc/sys/kernel/pid_max来提高上限。
<2>PID保存在进程描述符中。

2、内核中，访问任务通常需要获得指向其task_struct的指针。
<1>current宏查找当前正在运行进程的进程描述符。

三、进程状态
1、进程描述符中的state域描述了进程的当前状态。系统中的每个进程都必然处于五种状态中的一种：
<1>TASK_RUNNING(运行)：进程是可执行的。它或者正在执行，或者在运行队列中等待执行。
	这是进程在用户空间中执行的唯一可能状态。这种状态也可应用到内核空间中正在执行的进程。

<2>TASK_INTERRUPTIBLE(可中断)：进程正在睡眠(也就是说它被阻塞)，等待某些条件的达成。一旦条件达成，内核就会把进程状态设置为运行。
	处于此状态的进程也会因为接收到信号而提前被唤醒并随时准备投入运行。
	
<3>TASK_UNINTERRUPTIBLE(不可中断)：就算是接收到信号也不会被唤醒，此状态的任务对信号不响应。其他与可中断状态相同。
	这个状态通常在进程必须在等待时不受干扰 或 等待事件很快就会发生时出现。
	
<4>_TASK_TRACED：被其他进程跟踪的进程

<5>_TASK_STOPPED(停止)：进程停止执行。进程没有投入运行也不能投入运行。
	通常发生在接收到SIGSTOP等信号时；此外调试期间收到任何信号，都会使进程进入这种状态。
	
四、设置进程当前状态
内核调用 set_task_state(task,state)
		 set_current_state(state)

五、进程上下文
1、当一个进程执行了系统调用或者触发了某个异常，它就陷入了内核空间。此时，称为内核"代表进程执行"并处于进程上下文中。
在此上下文中current宏是有效的。

2、系统调用和异常处理程序是对内核明确定义的接口。进程只有通过这些接口才能陷入内核执行。

六、进程家族树
1、进程之间存在明显的继承关系，所有的进程都是PID为1的init进程的后代。
	内核在系统启动的最后阶段启动init进程。该进程读取系统的初始化脚本，并执行其他的相关程序。
<1>init进程的进程描述符是作为init_task静态分配的。

2、进程间的关系存放在进程描述符中：
<1>指向父进程描述符的 parent指针  struct task_struct __rcu *parent;
<2>一个称为children的子进程链表   struct list_head children;

3、访问进程
<1>借助init_task
struct task_struct *task;
for(task=current; task!=&init_task; task=task->parent)

<2>任务队列本来就是双向循环链表
next_task()/prev_task()宏

<3>for_each_process(task)宏提供了依次访问整个任务队列的能力

3.3 进程创建
1、其他系统：许多其他系统都提供了产生(spawn)进程的机制，首先在新的地址空间里创建进程，读入可执行文件，最后开始执行。
2、Unix系统：将上述步骤分解到两个单独的函数(fork、exec)中去执行。
			 fork()通过拷贝当前进程创建一个子进程。子进程与父进程的区别仅在于PID、PPID和某些资源的统计量。
			 exec()负责读取可执行文件并将其载入地址空间开始运行。
			 
一、写时拷贝 http://www.cnblogs.com/biyeymyhjob/archive/2012/07/20/2601655.html
1、linux的fork()使用写时拷贝(copy-on-write)页实现，可以推迟甚至避免拷贝父进程数据的技术。
	内核此时并不复制整个进程的地址空间，而是让父进程和子进程共享同一个拷贝。
	
2、只有在需要写入时，数据才会复制，从而使各个进程有各自的拷贝。
	在页根本不会被写的情况下(如，fork之后立即调用exec)，它们就无须复制了。
	
二、fork
1、linux通过clone()系统调用实现fork()。
	fork、vfork、__clone库函数都根据各自需要的参数标志去调用clone()，然后由clone()去调用()。
	
2、do_fork完成创建中的大部分工作。该函数调用copy_process()函数，然后让进程开始运行。

三、vfork
1、除了不拷贝父进程的页表项外，vfork()与fork()的功能相同。
	子进程作为父进程的一个单独进程在它的地址空间里运行，父进程被阻塞，直到子进程退出或执行exec()。
	子进程不能向地址空间写入。
	
2、由于执行fork()时引入了写时拷贝页，并且明确了子进程先执行。vfork的好处就仅限于不拷贝父进程的页表项了。//页表项和页表是一个东西？？

3、理想情况下，系统最高不要调用vfork()，内核也不用实现它。完全可以把vfork()实现成一个普通的fork()。

4、vfork，如果一切执行顺利，子进程会在新的地址空间里运行，而父进程也恢复在原地址空间的运行。确实降低了开销。

3.4 线程在linux中的实现
1、从内核角度来说，没有线程这个概念，linux把所有线程都当做进程来实现。
	线程仅仅被视为一个与其他进程共享某些资源(如地址空间)的进程。每个线程都拥有唯一隶属于自己的task_struct。
	
2、其他系统(如：windows\sun solaris)的内核提供了专门支持线程的机制。相对于重量级的进程，线程被抽象成一种耗费较少资源，运行迅速的执行单元。
	而对于linux，线程知识一种进程间共享资源的手段，linux的进程本身就够轻量级了。
	
一、创建线程
1、线程的创建于普通进程的创建类似，只是要在调用clone()时需要传递一些参数标志来指明需要共享的资源：
<1>clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)
上述代码产生的结果和调用fork()差不多，只是父子俩共享地址空间、文件系统资源、文件描述符和信号处理程序。
换句话说，新建的进程和它的父进程就是流行的所谓线程。

<2>普通fork实现：
clone(SIGCHLD,0)

<3>vfork实现：
clone(CLONE_VFORK | CLONE_VM | SIGCHLD, 0)

2、clone参数标记          
#define CLONE_VM    0x00000100  /* set if VM shared between processes */                             |~ 父子进程共享地址空间                                      
#define CLONE_FS    0x00000200  /* set if fs info shared between processes */                        |~ 父子进程共享文件系统信息                                      
#define CLONE_FILES 0x00000400  /* set if open files shared between processes */                     |~ 父子进程共享打开的文件                                      
#define CLONE_SIGHAND   0x00000800  /* set if signal handlers and blocked signals shared */          |~ 父子进程共享信号处理函数及被阻断的信号                                      
#define CLONE_PTRACE    0x00002000  /* set if we want to let tracing continue on the child too */    |~ 继续调试子进程                                      
#define CLONE_VFORK 0x00004000  /* set if the parent wants the child to wake it up on mm_release */  |~ 调用vfork，父进程准备睡眠，等待子进程将其唤醒                                      
#define CLONE_PARENT    0x00008000  /* set if we want to have the same parent as the cloner */       |~ 指定子进程与父进程拥有同一个父进程                                     
#define CLONE_THREAD    0x00010000  /* Same thread group? */                                         |~ 父进程放入相同的线程组                                      
#define CLONE_NEWNS 0x00020000  /* New namespace group? */                                           |~ 为子进程创建新的命名空间                                      
#define CLONE_SYSVSEM   0x00040000  /* share system V SEM_UNDO semantics */                          |~ 父子进程共享System V SEM_UNDO语言                                      
#define CLONE_SETTLS    0x00080000  /* create a new TLS for the child */                             |~ 为子进程创建新的TLS(thread-local storage)                                      
#define CLONE_PARENT_SETTID 0x00100000  /* set the TID in the parent */                              |~ 设置父进程的TID                                      
#define CLONE_CHILD_CLEARTID    0x00200000  /* clear the TID in the child */                         |~ 清除子进程的TID                                      
#define CLONE_DETACHED      0x00400000  /* Unused, ignored */                                        |~                                       
#define CLONE_UNTRACED      0x00800000  /* set if the tracing process can't force CLONE_PTRACE on thi|~                                       
#define CLONE_CHILD_SETTID  0x01000000  /* set the TID in the child */                               |~ 设置子进程的TID                                     
/* 0x02000000 was previously the unused CLONE_STOPPED (Start in stopped state)                       |~                                       
 and is now available for re-use. */                                                               |~                                       
#define CLONE_NEWUTS        0x04000000  /* New utsname group? */                                     |~                                       
#define CLONE_NEWIPC        0x08000000  /* New ipcs */                                               |~                                       
#define CLONE_NEWUSER       0x10000000  /* New user namespace */                                     |~                                       
#define CLONE_NEWPID        0x20000000  /* New pid namespace */                                      |~                                       
#define CLONE_NEWNET        0x40000000  /* New network namespace */                                  |~                                       
#define CLONE_IO        0x80000000  /* Clone io context */  

二、内核线程 (kernel thread) --独立运行在内核空间的标准进程
1、内核线程与普通进程的区别：内核线程没有独立的地址空间(实际上指向地址空间的mm指针被设置为NULL)
							 它们只在内核空间运行，从来不切换到用户空间去。

2、内核线程只能由其他内核线程创建。kthread_create()-wake_up_process() //创建+运行
								   kthread_run() //创建并运行
	
3、内核线程一直运行直到调用do_exit()，或者内核的其他部分调用kthread_stop()。
	传递给kthread_stop的参数是kthread_create返回的task_struct结构地址。


3.5 进程终结
1、内核释放进程占有的资源，并通知其父进程。
<1>一般来说，进程的析构是自身引起的。它发生在进程调用exit()系统调用时。
	即可能是显示调用，也可能是隐式地从某个程序的主函数返回。(C语言编译器会在main函数返回点后面放置调用exit的代码)

<2>当进程接收到它既不能处理也不能忽略的信号或异常时，也可能被动地终结。

<3>不管进程如何终结，该任务大部分都依靠do_exit()完成。

	do_exit调用exit_notify()向父进程发送信号，给子进程重新找养父。养父为线程组(子进程所在的线程组)中的其他线程或者init进程。
	do_exit调用schedule()切换到新的进程。因为处于EXIT_ZOMBIE状态的进程不会再被调度，因此这是该进程执行的最后一段代码。
	do_exit永远不返回。
	
<4>do_exit()执行完毕之后。与进程相关的所有资源都被释放掉了(假设该进程是这些资源的唯一使用中)。
1)进程不可运行(实际上也没有地址空间让它运行)，并处于EXIT_ZOMBIE退出状态。
2)进程所占用的所有内存就是 内核栈、thread_info结构、task_struct结构
3)此时进程存在的唯一目的就是向它的父进程提供信息。父进程检索到信息后，或者通知内核那是无关的信息后，由进程所持有的剩余内存被释放，归还给系统使用。

一、删除进程描述符
1、wait()这一族函数都是通过唯一的系统调用wait4()来实现的。
	它的标准动作是挂起调用它的进程，直到其中的一个子进程退出，此时函数会返回该子进程的PID。
	此外，调用该函数时提供的指针会包含子进程退出时的退出代码。

2、当最终需要释放进程描述符时，release_task()会被调用。

二、孤儿进程造成的进退维谷
1、如果父进程在子进程之前退出，必须有机制保证子进程能找到一个新的父亲。否则子进程就会在自身退出时永远处于僵死状态，白白消耗内存。
1)解决方法是 给子进程在当前线程组内找一个线程作为父亲。如果不行，就让init作为它们的父进程。
2)do_exit--exit_notify--forget_original_parenet--find_new_reaper 执行寻父过程。

2、遍历两个链表：子进程链表children
				 ptrace子进程链表ptraced
	给每个子进程设置新的父进程。
	
3、当一个进程被跟踪时，它的临时父亲设定为调试进程，此时，如果它的父进程退出了，系统会为它和它的所有兄弟重新找一个父进程。
	//对应ptrace子进程链表
	
4、init进程会例行调用wait()来检查其子进程，清除所有与其相关的僵死进程。


第四章 进程调度
进程调度程序：确保进程有效工作的一个内核子系统。
			  在可运行态进程之间分配有限的处理器时间资源的内核子系统。
			  是像linux这样的多任务操作系统的基础。
			  
4.1 多任务
1、多任务操作系统就是能同时并发地交互执行多个进程的操作系统。
<1>无论是在单处理器或多处理器的机器上，多任务操作系统都能使多个进程处于堵塞或者睡眠状态。
	这些任务尽管位于内存，但并不处于可运行态。
	这些进程利用内核阻塞自己，直到某件事情发生。
	
<2>多任务系统可以分为
1)抢占式多任务
linux提供了抢占式多任务模式。由调度程序决定什么时候停止一个进程的运行，以便其他进程能够得到执行机会。这种强制的挂起动作就叫抢占。
	
进程在被抢占之前能够运行的时间是预先设置好的，叫进程的时间片。也就是分配给每个可运行进程的处理器时间段。

2)非抢占式多任务
除非进程自己主动停止，否则它会一直执行。
进程主动挂起自己的操作称为让步。理想情况下，进程通常做出让步，以便让每个可运行进程享有足够的处理器时间。

该模式下，调度程序无法对每个进程该执行多长时间做出统一规定。

4.2 Linux的进程调度
完全公平调度算法 CFS

4.3 策略
策略决定了调度程序在何时让什么进程运行。调度器的策略往往决定了系统的整体印象，并且，还要负责优化使用处理器时间。

一、I/O消耗型和处理器消耗型的进程
1、I/O消耗型进程
<1>进程的大部分时间用来提交I/O请求或是等待I/O请求。
	这样的进程经常处于可运行状态，但通常都是运行短短一会。
	
2、处理器消耗型进程
<1>把大量时间耗费在执行代码上。除非抢占，否则通常他们都是一直不停地运行。

<2>从系统响应角度考虑，调度器不应该经常让它们运行。
	
3、调度策略需要在两个矛盾的目标间寻找平衡：进程响应迅速(响应时间短)和最大系统利用率(高吞吐量)。
Unix系统的调度程序更倾向于I/O消耗型程序，从而提供更好的程序响应速度。
Linux为了保证交互式应用和桌面系统的性能，所以对进程的响应做了优化(缩短响应时间)，更倾向于优先调度I/O消耗型进程。

二、进程优先级
1、调度算法中最基本的一类就是基于优先级的调度。
<1>这是一种根据进程的价值和其对处理器时间的需求来对进程分级的想法。
<2>通常做法是优先级高的进程先运行，低的后运行，相同优先级的进程按照轮转方式进行调度。
<3>在某些系统中，优先级高的进程使用的时间片也较长。
	调度程序总是选择时间片未用尽且优先级最高的进程运行。
	用户和系统都可以通过设置进程的优先级来影像系统的调度。
	
2、linux采用了两种不同的优先级范围。
<1>nice值，范围从-20到19，默认为0。越大的nice值意味着优先级更低。(nice值意味着对系统其他进程的“优待”。)
1)低nice值的进程可以获得更多的处理器时间。linux系统中nice值代表时间片的比例。

<2>实时优先级，其值是可配置的，默认情况下变化范围[0,99]。
1)与nice值相反，越大的实时优先级数值意味着进程优先级越高。
2)任何实时进程的优先级都高于普通的进程。也就是说实时优先级和nice优先级处于互不相交的两个范畴。

<3>ps -eo state,uid,pid,ppid,rtprio,time,comm.
查看系统中的进程列表，以及它们的实时优先级(位于RTPRIO列下)，如果对应显示-，说明它不是实时进程。

三、时间片
1、在其他系统中时间片有时也称为量子或处理器片。

2、时间片是一个数值，它表明进程在被抢占前所能持续运行的时间。调度策略必须规定一个默认的时间片。
<1>任何长时间片都将导致系统交互表现欠佳。所以默认时间片很短，如10ms。
1)linux的CFS调度器并没有直接分配时间片到进程，它是将处理器的使用比例划分给了进程。
	这样，进程所获得的处理器时间其实是和系统负载密切相关的。

2)这个比例进一步还受到进程nice值的影响，nice值作为权重将调整进程所使用的处理器时间使用比。

<2>当一个进程进入可运行态，它就被允许投入允许。
1)在多数操作系统中，是否要将一个进程立刻投入运行(也就是抢占当前进程)，是完全由进程优先级和是否有时间片决定的。
2)而在linux系统中，使用新的CFS调度器，其抢占时机取决于新的可运行程序消耗了多少处理器使用比。
	如果消耗的使用比小于当前进程，则新进程立刻投入运行，抢占当前进程。否则，将推迟运行。
	
四、调度策略的活动
文件编辑器程序和视频编码程序的例子。

4.4 linux调度算法
一、调度器类 scheduler classes
1、linux调度器是以模块方式提供的，这样的目的是允许不同类型的进程可以有针对性地选择调度算法。这种模块化结构称为调度器类。
<1>它允许多种不同的可动态添加的调度算法并存，调度属于自己范畴的进程。
1)每一个调度器都有一个优先级。基础的调度器代码会按照优先级顺序遍历调度类，拥有一个可执行进程的最高优先级的调度器类胜出，去选择下面要执行的那个程序。

<2>完全公平调度(CFS)是一个针对普通进程的调度类，在linux中称为SCHED_NORMAL

二、Unix系统中的进程调度
1、要将nice值映射到时间片，就必然需要将nice单位值对应到处理器的绝对时间。这样将导致进程切换无法最优化进行。

2、相对nice值。
<1>nice值通常都是用相对值。nice系统调用是在原值上增加或减少，而不是在绝对值上操作。
<2>	0 -- 100ms 
	1 --  95ms
   18 --  10ms
   19 --   5ms
也就是说，把进程的nice值减小1所带来的效果极大的取决于nice的初始值。

3、如果执行nice值到时间片的映射，我们需要能分配一个绝对时间片，而且这个绝对时间片必须能在内核测试范围内。
	在多数操作系统中，这就要求时间片必须是定时器节拍的整数倍。
	
4、分配绝对的时间片引发的固定的切换频率，给公平性造成了很大变数。

三、公平调度
1、CFS的出发点基于一个简单的理念：进程调度的效果应如同系统具备一个理想中的完美的多任务处理器。
<1>每个进程将获得1/n的处理器时间，n是指可运行的进程的数目。
<2>同时我们可以调度给它们无限小的时间周期，所以在任何可测量的周期内，我们给予n个进程中的每个进程同样多的运行时间。

2、上述理想模型并非现实，因为我们无法在一个处理器上真正同时运行多个进程。
而且，如果每个进程运行无限小的时间周期也不是高效的。因为调度时进程抢占会带来一定的代价。
将一个进程换出，另一个换入本身就有消耗，同时还会影响到缓冲的效率。

<1>CFS首先要确保系统性能不受损失。
1)CFS允许每个进程运行一段时间、循环轮转、选择运行最少的进程作为下一个运行进程。
2)CFS在所有可运行进程总数基础上计算出一个进程应该运行多久，而不是依靠nice值来计算时间片。
3)nice值在CFS中被作为进程获得的处理器运行比的权重。
	每个进程都按其权重在全部可运行进程中所占的比例的"时间片"来运行。
4)目标延迟：CFS为完美多任务中的无限小调度周期的近似值设立的一个目标。   //调度周期：所有任务调度一遍的时间
	越小的调度周期将带来越好的交互性，同时也更接近完美的多任务。
5)最小粒度：CFS引入的每个进程获得时间片的底线。默认情况下这个值是1ms。确保切换消耗被限制在一定范围内。

3、绝对的nice值不再影响调度决策，只有相对值才会影响处理器时间的分配比例。
	0   5    //nice值为5的进程的权重将是默认nice进程的1/3
   10  15    //nice值为15的进程的权重将是nice值为10的进程的1/3
   
   任何进程所获得的处理器时间是由它自己和其他所有可运行进程nice值的相对差值决定的。
   nice值对时间片的作用不再是算术加权，而是几何加权。
   任何nice值对应的绝对时间不再是一个绝对值，而是处理器的使用比。(这样可以在目标延迟内进行多个进程的切换)
	CFS称为公平调度器是因为它确保每个进程公平的处理器使用比。
	
4.5 Linux调度的实现  kernel/sched/fair.c

一、时间记账
<1>所有的调度器都必须对进程运行时间做记账。
<2>多数Unix系统，分配一个时间片给每一个进程。那么当每次系统时钟节拍发生时，时间片都会被减少一个节拍周期。
当一个进程的时间片被减少到0时，它就会被另一个时间片尚未减到0的可运行进程抢占。

1、调度器实体结构  //sched_entity结构体
<1>它作为一个名为se的成员变量，嵌入在进程描述符task_struct结构内。
<2>用于追踪进程运行时间记账。

2、虚拟实时
<1>结构体sched_entity中的vruntime变量存放进程的虚拟运行时间(花在运行上的时间和)，
	该运行时间的计算是经过了所有可运行进程总数的标准化(或者说被加权)的。
	
<2>单位为 ns，因此vruntime与定时器节拍不再相关。

<3>CFS使用vruntime变量来记录一个程序到底运行了多次时间以及它还应再运行多久。

<4>sched/fair.c文件中的update_curr()函数实现了该记账功能。
1)update_curr()是由系统定时器周期性调用的，无论进程处于可运行态，还是被阻塞处于不可运行态。
	根据这种方式，vruntime可以准确地测量给定进程的运行时间，而且可以知道谁应该是下一个运行的进程。

二、进程选择
<1>CFS试图利用一个简单的规则去均衡进程的虚拟运行时间：
	当CFS需要选择下一个运行进程时，它会挑一个具有最小vruntime的进程。

<2>CFS调度算法的核心：
	选择具有最小vruntime的任务。
	
<3>CFS使用红黑树来组织可运行进程队列，并且利用其迅速找到最小vruntime值的进程。 //CFS只是普通进程的调度类，可以预见每个调度类都对应一棵红黑树。

1、挑选下一个任务
<1>假设有一个红黑树(rbtree)存储了系统中所有可运行进程，其中节点的键值便是可运行进程的虚拟运行时间。
<2>CFS调度器选取待运行的下一个进程，是所有进程中vruntime最小的那一个，它对应的便是在树中最左侧的叶子结点。
<3>kernel/sched/fair.c	__pick_next_entity() 实现这一过程。
	但实际上该函数本身并不会遍历红黑树去找到最左叶子节点，因为该值已经缓存在rb_leftmost字段中。

2、向树中加入进程
<1>CFS将进程加入rbtree，以及缓存最左叶子节点，发生在进程变为可运行状态(被唤醒)或者通过fork()调用第一次创建进程时。
__enqueue_entity()函数实现。

3、从树中删除进程
删除动作发生在进程堵塞(变为不可运行态)或者终止时(结束运行)。
__dequeue_entity()函数实现。

三、调度器入口
1、调度进程的主要入口函数是schedule()。
<1>它是内核其他部分用于调度进程调度器的入口：选择哪个进程可以运行，何时将其投入运行。
<2>schedule()通常都需要和一个具体的调度类相关联。也就是说，它会找到一个最高优先级的调度类。
	该调度类需要有自己的可运行队列，然后找到下一个要运行的进程。
<3>schedule()会调用pick_next_task()，后者会以优先级为序，从高到低，依次检查每个调度类。
	并且从最高优先级的调度类中选择最高优先级的进程。
<4>pick_next_task()会调用pick_next_entity(),而后者会调用__pick_next_entity()。

四、睡眠和唤醒
1、休眠(被阻塞)的进程处于一个特殊的不可执行状态。进程休眠有多种原因，但肯定都是为了等待一些事件。
<1>常见情况：文件I/O。
<2>无论哪种情况，内核的操作都是：进程把自己标记成休眠状态，从可执行红黑树中移出，放入等待队列。
	然后调用schedule()选择和执行一个其他进程。
<3>休眠有两种相关的进程状态：TASK_INTERRUPTIBLE和TASK_UNINTERRUPTIBLE。两种状态的进程位于同一个等待队列上，等待某些事件，不能够运行。
	它们的唯一区别是处于后者状态的进程会忽略信号；而处于前置状态的进程会被提前唤醒并响应该信号。
	
2、唤醒的过程与休眠相反：进程被设置为可执行状态，然后再从等待队列中移到可执行红黑树中。

3、等待队列
<1>休眠通过等待队列进行处理。等待队列是由等待某件事件发生的进程组成的简单链表。
	内核用wake_queue_head_t来表示等待队列。
	
4、唤醒
<1>唤醒通过函数wake_up()进行，它会唤醒指定的等待队列上的所有进程。
	调用函数try_to_wake_up()，该函数负责将进程设置为TASK_RUNNING状态，调用enqueue_task()将此进程放入红黑树中。
	如果被唤醒的进程优先级比当前正在执行的进程的优先级高，还要设置need_resched标志。
<2>通常哪段代码促使等待条件达成，它就要负责随后调用wake_up()函数。

4.6 抢占和上下文切换
1、上下文切换，也就是从一个可执行进程切换到另一个可执行进程。
<1>context_switch()函数负责处理。定义在/kernel/sched/core.c。
	每当一个新的进程被选出来准备投入运行时，schedule()就会调用该函数。
1)调用switch_mm()，把虚拟内存从上一个进程映射切换到新的进程中。
2)调用switch_to()，从上一个进程的处理器状态切换到新进程的处理器状态。
	包括保存、恢复栈信息和寄存器信息，还有其他任何与体系结构相关的状态信息，都必须以每个进程为对象进行管理和保存。
	
2、内核必须知道在什么时候调用schedule()。如果仅靠用户程序代码显式地调用schedule()，他们可能就会永远地执行下去。
<1>内核提供了一个need_resched标志来表明是否需要重新执行一次调度。
1)当某个进程应该被抢占时，schedule_tick()就会设置这个标志。
2)当一个优先级高的进程进入可执行状态时，try_to_wake_up()也会设置这个标志。

内核检查该标志，确认其被设置，调用schedule()来切换到一个新的进程。
该标志对于内核来讲是一个信息，它表明有其他进程应该被运行了，要尽快调用调度程序。

<2>再返回用户空间以及从中断返回时，内核也会检查need_resched标志。如果已经被设置，内核会在继续执行之前调用调度程序。

<3>每个进程都包含一个need_resched标志，这是因为访问描述符内的数值要比访问一个全局变量快。
(current宏速度很快并且描述符通常都在高速缓存中)
1)2.6版本之后，它被移到thread_info结构体中，用一个特别的标志变量中的一位来表示。

一、用户抢占
1、内核即将返回用户空间时，如果need_resched标志被设置，会导致schedule()被调用，此时就会发生用户抢占。

2、从中断处理程序或系统调用返回的返回路径都是跟体系结构相关的，在entry.S文件中通过汇编语言来实现。
<1>entry.S不仅包含内核入口部分代码，也包含内核退出部分代码。

3、用户抢占发生在：
<1>从系统调用返回用户空间时；
<2>从中断处理程序返回用户空间时。

二、内核抢占
1、linux完整地支持内核抢占。只要重新调度是安全的，内核就可以在任何时间抢占正在执行的任务。
<1>什么时候重新调度是安全的：
	只要没有持有锁，内核就可以抢占。锁是非抢占区域的标志。
	由于内核支持SMP，所以，如果没有持有锁，正在执行的代码就是可重新导入的，也就是可以抢占的。
	
<2>为了支持内核抢占所做的第一处变动，就是为每个进程的thread_info引入preempt_count计数器。
	该计数器初始值为0，每当使用锁时数值+1，释放锁时数值-1。数值为0时，内核就可以执行抢占。
	
<3>如果内核中的进程被阻塞了，或内核显式地调用了schedule()，内核抢占也会显示地发生。
	这种形式的内核抢占从来都是受支持的，因为根本无须额外的逻辑来保证内核可以安全地被抢占。
	(如果代码显示地调用schedule(),那么它应该清楚自己是可以安全地被抢占的。)

2、内核抢占会发生在:
<1>中断处理程序正在执行，且返回内核空间之前；
<2>内核代码再一次具有可抢占性的时候；
<3>如果内核中的任务显式地调用schedule();
<4>如果内核中的任务阻塞(这统一也会导致调用schedule())。

4.7 实时调度策略
1、linux提供了两种实时调度策略：SCHED_FIFO和SCHED_RR。
                  普通、非实时的调度策略是：SCHED_NORMAL。-- CFS
这些实时调度策略并不被CFS调度器来管理，而是被一个特殊的实时调度器管理。

<1>SCHED_FIFO  //先入先出的调度算法
1)它不使用时间片，处于可运行状态的SCHED_FIFO级的进程会比任何SCHED_NORMAL级的进程都先得到调度。
2)一旦一个SCHED_FIFO级进程处于可执行状态，就会一直执行，直到它自己受阻塞或显式地释放处理器为止。
3)只有更高优先级的SCHED_FIFO或者SCHED_RR任务才能抢占SCHED_FIFO任务。
4)如果有两个或多个同优先级的SCHED_FIFO级进程，它们会轮流执行，但依然只有在它们愿意让出处理器时才会退出。

<2>SCHED_RR
1)SCHED_RR与SCHED_FIFO大体相同，只是SCHED_RR级的进程在耗尽实现分配给它的时间后就不能再继续执行了。
	也就是说SCHED_RR是带有时间片的SCHED_FIFO,这是一种实时轮流调度算法。
2)当SCHED_RR任务耗尽它的时间片时，在同一优先级的其他实时进程会被轮流调度。
3)时间片只是用来重新调度同一优先级的进程。对于SCHED_FIFO进程，高优先级总是立即抢占低优先级，
	但低优先级进程决不能抢占SCHED_RR任务，即使它的时间片耗尽。

2、实时优先级范围从0到MAX_RT_PRIO-1，默认情况下，MAX_RT_PRIO为100。[0,99]
   
   SCHED_NORMAL级进程的nice值共享了这个取值空间，它的取值范围从MAX_RT_PRIO到(MAX_RT_PRIO+40)。
	也就是说默认情况下nice值[-20,19],直接对应的是[100,139]的实时优先级。
   

4.8 与调度相关的系统调用
1、linux提供了一个系统调用族，用于管理与调度程序相关的参数。
<1>这些系统调用可以用来操作和处理进程优先级、调度策略以及处理器绑定。同时还提供了显式地将处理器交给其他进程的机制。

一、与调度策略和优先级相关的系统调用
<1>sched_setscheduler/sched_getscheduler 设置/获取进程的调度策略(task_struct中的policy)、实时优先级(task_struct中的rt_priority)。
<2>sched_setparam/sched_getparam 设置/获取进程的实时优先级(sched_param中的rt_priority)。
<3>sched_get_priority_max/sched_get_priority_min 返回给定调度策略的最大(MAX_USER_RT_PRIO-1)和最小(1)优先级。
<4>对于一个普通的进程，nice函数可以将给定进程的静态优先级增加一个给定的量。只有root用户才能调用它时使用负值。
nice函数会调用内核的set_user_nice函数，设置进程task_struct中的static_prio和prio。

二、与处理器绑定有关的系统调用
<1>linux调度程序提供了强制的处理器绑定机制。也就是说，虽然它尽力通过一种软的(或者说自然的)亲和性试图使进程尽量在同一个处理器上运行，
但它也允许用户强制指定“这个进程无论如何都必须在这些处理器上运行”。

<2>task_struct中的cpus_allowed这个为掩码标志，该标志的每一位对应一个系统可用的处理器。

三、放弃处理器时间
linux通过sched_yield系统调用，提供了一种让进程显式地将处理器时间让给其他等待执行的进程的机制。
<1>通过将当前进程从活动队列中移到过期队列中来实现。
<2>不仅抢占了该进程，并将其放入优先级队列的最后面；还将其放入过期队列中，这样确保在一段时间内都不会再被执行。
<3>由于实时进程不会过期，所以，它只能被移到其优先级队列的最后面；但不会放到过期队列中。
<4>在linux早期版本中，sched_yield语义有所不同，进程只会被放置到其优先级队列的末尾，放弃的时间往往不会太长。
<5>内核代码为了方便，可以直接调用yield函数。(先要确定进程确实处于可执行状态，然后在调用sched_yield)
用户空间的应用程序直接使用sched_yield系统调用即可。

第五章 系统调用
在现代操作系统中，内核提供了用户进程与内核进行交互的一组接口。提供这些接口主要是为了保证系统的稳定可靠，避免应用程序肆意妄为。

5.1 与内核通信
在linux中，系统调用是用户空间访问内核的唯一手段；除异常和陷入外，它们是内核唯一的合法入口。

5.2 API、POSIX和C库
1、一般情况下，应用程序通过在用户空间实现的应用编程接口(API)而不是直接通过系统调用来编程。
<1>API并不需要与内核提供的系统调用对应。
<2>API可以实现成一个系统调用，也可以通过调用多个系统调用来实现，甚至可以完全不使用任何系统调用。

2、POSIX、API、C库以及系统调用之间的关系
调用printf() -> C库中的printf() -> C库中的write() -> write()系统调用
 应用程序							   					  内核

<1>在Unix世界中，最流行的应用编程接口(API)是基于POSIX标准的。POSIX是由IEEE的一组标准组成，其目标是提供一套大体上基于Unix的可移植操作系统标准。
<2>在大多数Unix系统上，根据POSIX定义的API函数和系统调用之间有着直接关系。实际上，POSIX标准就是仿照早期Unix系统的接口建立的。
<3>Linux系统调用像大多数Unix系统一样，作为C库的一部分提供。
	C库实现了Unix系统的主要API，包括标准C库函数和系统调用接口。此外，C库提供了POSIX的绝大部分API。
<4>程序员只需要跟API打交道；相反，内核只跟系统调用打交道。
	
5.3 系统调用 syscall
1、要访问系统调用，通常通过C库中定义的函数调用来进行。它们通常都需要定义0个、1个或几个输入参数，而且可能产生一些副作用。
<1>绝大部分调用都会产生副作用，会使系统状态发生某种变化。
<2>系统调用会通过一个long类型的返回值来表示成功或者失败；用一个负数返回值表明错误；用0表明成功。
<3>系统调用在出现错误时，C库会把错误码写入errno全局变量。通过调用perror()库函数，可以把该变量翻译成用户可以理解的错误字符串。

2、SYSCALL_DEFINE0 宏，定义了一个无参数的系统调用(也因此，这里为数字0)。
	
3、#define SYSCALL_DEFINE0(sname)                  \
     SYSCALL_METADATA(_##sname, 0);              \
     asmlinkage long sys_##sname(void)

	 asmlinkage log sys_getpid(void)
<1>asmlinkage限定词，这是一个编译指令，通知编译器仅从栈中提取该函数的参数。所有的系统调用都必须有这个限定词。
<2>函数返回long。
	为了保证32位和64位系统的兼容，系统调用在用户空间和内核空间有着不同的返回值类型，用户空间int，内核空间long。
<3>系统调用getpid()在内核中被定义为sys_getpid()。这是linux中所有系统调用都必须遵守的命令规则。
	
一、系统调用号
1、在linux中，每个系统调用都被赋予一个系统调用号。通过这个独一无二的系统调用号就可以关联到该系统调用。
	当用户空间进程执行一个系统调用时，是用系统调用号来指明的，进程不会提及系统调用的名称。
	
<1>系统调用号一旦分配就不能再有任何变更，否则编译好的应用程序就会崩溃。
<2>如果一个系统调用被删除，它所占用的系统调用号也不允许被回收利用。
	否则，之前编译过的代码会调用这个系统调用，但事实上却调用的另一个系统调用。
<3>linux有一个“未实现”系统调用sys_ni_syscall()，它除了返回-ENOSYS外不做任何其他工作，这个错误就是专门针对无效的系统调用设计的。
	如果一个系统调用被删除，或者变得不可用，这个函数就要负责“填补空缺”。
	
2、内核记录了系统调用表中所有已注册过的系统调用列表，存储在sys_call_table中。
	这个表为每一个有效的系统调用指定了唯一的系统调用号。
	
二、系统调用的性能
Linux系统调用比其他许多操作系统执行得要快。
Linux很短的上下文切换时间是一个重要原因，进出内核都被优化的简洁高效；另一个原因是系统调用处理程序和每个系统调用本身也都非常简洁。

5.4 系统调用处理程序
1、用户空间的程序无法直接执行内核代码，它们不能直接调用内核空间中的函数。
<1>应用程序应该以某种方式通知系统，告诉内核自己需要执行一个系统调用，希望系统切换到内核态，这样内核就可以代表应用程序在内核空间执行系统调用。
<2>通知的机制是靠软中断实现的:通过引发一个异常来促使系统切换到内核态去执行异常处理程序。此时的异常处理程序实际上就是系统调用处理程序。
	
一、指定恰当的系统调用
1、所有的系统调用陷入内核的方式都一样，仅仅陷入内核空间是不够的。因此必须把系统调用号一并传给内核。
<1>x86上，系统调用号是通过eax寄存器传递给内核的。
<2>系统调用处理函数system_call()通过将给定的系统调用与NR_syscalls做比较来检查其有效性。如果它大于或等于NR_syscalls，该函数就返回-ENOSYS。
	否则就执行相应的系统调用。call *sys_call_table(,%rax,8)
	
二、参数传递
1、除了系统调用号以外，大部分系统调用还需要一些外部的参数输入。所以，在发生陷入时，需要把这些参数从用户空间传递给内核。	
<1>最简单的办法就是把这些参数也放在寄存器中。x86-32系统中，用五个寄存器分别存放前五个参数。	(用一个单独的寄存器存放指向所有这些参数在用户空间地址的指针)
<2>给用户空间的返回值也通过寄存器eax传递。

5.5 系统调用的实现
一、实现系统调用
1、linux中不提倡采用多用途的系统调用，每个系统调用应该有一个明确的用途。
2、可移植性和健壮性。

二、参数验证
1、系统调用必须仔细检查它们所有的参数是否合法有效。
	系统调用在内核空间执行，如果任由用户将不合法的输入传递给内核，那么系统的安全和稳定将会面临极大的考研。
	
2、最重要的一种检查是检查用户提供的指针是否有效。
<1>指针指向的内存区域属于用户空间。进程决不能哄骗内核去读内核空间的数据；
<2>指针指向的内存区域在进程的地址空间。进程决不能哄骗内核去读其他进程的数据；
<3>如果是读，该内存应被标记为可读；如果是写，该内存应被标记为可写；如果是执行，该内存应被标记为可执行。进程决不能绕过内存访问限制。

3、内核提供了两个方法来完成必须的检查和内核空间与用户空间之间数据的来回拷贝。
<1>copy_to_user()  //向用户空间写入数据
<2>copy_from_user()   //从用户空间读取数据	
	
<3>如果执行失败，这两个函数返回的都是没能完成拷贝的数据的字节数。如果成功，则返回0。
	当出现上述错误时，系统调用返回标准-EFAULT。
	
<4>这两个方法都有可能引起阻塞。当包含用户数据的页被换出到硬盘上而不是在物理内存上的时候，这种情况就会发生。
	此时进程就会休眠，直到缺页处理程序将该页从硬盘换回物理内存。

<5>suser()
   capable()
   
六、系统调用上下文
1、内核在执行系统调用时处于进程上下文。current指针指向当前任务，即引发系统调用的那个进程。
<1>在进程上下文中，内核可以休眠并且可能被抢占。
<2>在进程上下文中能够被抢占其实表明，像用户空间一样，当前的进程同样可以被其他进程抢占，因为新的进程可以使用相同的系统调用。
	所以要保证系统调用是可以重入的。
	
2、在系统调用返回时，控制权仍在system_call()中，它最终负责切换到用户空间，并让用户进程继续执行下去。

一、绑定一个系统调用的最后步骤
1、增加系统调用表项
2、在asm/unistd.h中增加系统调用号
3、将系统调用的实现编入内核映像。只要放入kernel/下的一个相关文件即可。
	
二、从用户控件访问系统调用
1、linux本身提供了一组宏，用于直接对系统调用进行访问。_syscalln(),其中n的范围从0到6，代表需要传递给系统调用的参数个数。
eg:open()系统调用：
	long open(const char* filename, int flags, int mode)
   不依靠库支持，直接调用此系统调用：
    #define NR_open	5  //在asm/unistd.h中定义，是系统调用号
	_syscall3(long, open, const char *, filename, int, flags, int, mode)  //对于每个宏来说，都有2+2*n个参数。
这样，应用程序就可以直接使用open()了。
	
三、为什么不通过系统调用方式实现

第六章 内核数据结构
6.1 链表
一、单向链表和双向链表
1、单向链表中的一个元素
struct list_element {
		void *data;			       /*有效数据*/
		struct list_element *next; /*指向下一个元素的指针*/
};

2、双向链表中的一个元素
struct list_element {
		struct list_element *next; /*指向下一个元素的指针*/
		void *data;				   /*有效数据*/
		struct list_element *prev; /*指向前一个元素的指针*/
};


通常链表的最后一个元素不再有下一个元素，所以将链表尾元素中的向后指针设为NULL，链表头元素的向前指针设为NULL，以表示是最后一个元素。

二、环形链表
1、链表末尾元素并不指向特殊值NULL，相反，它指回链表的首元素。这种链表首尾相连，所以被称为环形链表。
2、环形链表也存在双向链表和单向链表两种形式。
	在环形双向链表中，首节点的向前指针指向尾节点。
	
3、因为环形双向链表提供了最大的灵活性，因此linux内核的标准链表就是采用环形双向链表形式实现的。

三、沿链表移到
1、沿链表移动只能是线性移动；如果需要随机访问数据，一般不考虑使用链表。
	使用链表存放数据的理想情况是，需要遍历所有数据或者需要动态加入和删除数据。
	
2、为了方便快速地找到链表的头，有时候首元素会用一个特殊的指针表示。该指针称为头指针。

四、linux内核中的实现
linux内核方式与众不同，它不是将数据结构塞入链表，而是将链表节点塞入数据结构。

1、链表数据结构  linux/types.h  linux/list.h
struct list_head {
		struct list_head *next;
		struct list_head *prev;
};

<1>container_of() 宏，可以很方便地从链表指针找到父结构中包含的任何变量。
	这是因为在C语言中，一个给定结构中的变量偏移在编译时地址就被ABI(应用程序二进制接口)固定下来了。
	
2、定义一个链表


6.2 队列  kernel/kfifo.c   //环形队列
一、kfifo
<1>入口偏移：下一次入队列时的位置
   出口偏移：下一次出队列时的位置，出口偏移总是小于或等于入口偏移。

<2>enqueue 入队列   拷贝数据到队列的入口偏移位置，入口偏移随之加上推入的元素数目。
   dequeue 出队列   从出口偏移处拷贝数据，出口偏移随之减去摘取的元素数目。

6.3 映射

6.4 二叉树
---------------------------------------------------------------------------------
0、基本定义
<1>结点的度：结点拥有的子树数目称为结点的度；
<2>树的度：树内各个结点的度的最大值，即为树的度；

1、斜树
所有结点都只有左子树的二叉树叫左斜树；所有结点都只有右子树的二叉树叫右斜树。
2、满二叉树
在一棵二叉树中，如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树。
3、完全二叉树
对一颗具有n个结点的二叉树按层序编号，如果编号为i(1<=i<=n)的结点与同样深度的满二叉树中编号为i的结点在二叉树中位置完全相同，则这颗二叉树称为完全二叉树。
性质：<1>叶子结点只能出现在最下两层；
      <2>最下层的叶子一定集中在左边连续位置；
      <3>倒数第二层若有叶子结点，一定都在右边且连续；
      <4>如果结点度为1，则该节点只有左孩子，即不存在只有右子树的情况；
      <5>同样结点树的二叉树，完全二叉树的深度最小。
4、平衡二叉树 (AVL树)
是一种二叉排序树，其中每个结点的左子树和右子树的高度差至多等于1。
----------------------------------------------------------------------------------

一、二叉搜索树(BST)
BST是一个节点有序的二叉树：
<1>树的左分支节点值都小于根节点值
<2>树的右分支节点值都大于跟节点值
<3>所有子树也都是二叉搜索树

二、自平衡二叉搜索树
所有叶子节点的深度差不超过1的二叉搜索树。
1、红黑树
<1>红黑树是一种自平衡二叉搜索树。
1)所有节点要么红色要么黑色
2)叶子节点都是黑色
3)叶子节点不包含数据
4)所有非叶子节点都有两个子节点
5)如果一个节点是红色，则它的子节点都是黑色
6)在一个节点到其叶子节点的路径中，如果总是包含同样数目的黑色节点，则该路径相比其他路径是最短的。



6.6 算法复杂度
一、算法

二、大o符号
它是一个函数，其值从一个起始点之后总是超过我们所研究的函数的值，也就是说上限增长等于或快于我们研究的函数。特殊符号大o用于描述这种增长率。
f(x) = O(g(x)) 
完成f(x)的时间总是短于或等于完成g(x)的时间和任意常量的乘积。


第七章 中断和中断处理
7.1 中断
1、中断使得硬件得以发出通知给处理器。  敲击键盘--键盘控制器(控制键盘的硬件设备实体)发出一个中断，通知操作系统有键按下
<1>硬件设备产生中断时不考虑与处理器的时钟同步，也就是说中断可以随时产生。因此，内核随时可能因为新到来的中断而被打断。
<2>中断由硬件设备生成，直接送入中断控制器的输入引脚中。
	中断控制器是一个简单的电子芯片，其作用是将多路中断管线采用复用技术只通过一个和处理器相连接的管线与处理器通信。
<3>处理器一经检测到中断信号，便中断自己的当前工作，转而处理中断。此后，处理器会通知操作系统已经产生中断。
	这样，操作系统就可以对这个中断进行适当的处理了。
	
2、不同的设备对应的中断不同，每个中断都通过一个唯一的数字标志。操作系统给不同的中断提供不同的处理程序。
<1>这些中断值通常被称为中断请求(IRQ)线。每个IRQ线都会被关联一个数值量。
	硬件发出中断的目的是引起内核的关注。
	
3、异常
<1>异常的产生必须考虑与处理器时钟同步。因此，异常也常常成为同步中断。
<2>处理器执行到由于编程失误而导致的错误指令(被0除)时，或者在执行期间出现特殊情况(如缺页)，必须靠内核来处理的时候，
	处理器就会产生一个异常。
<3>中断(由硬件产生的异步中断)；异常(由处理器产生的同步中断)。

7.2 中断处理程序
1、在响应一个特定中断的时候，内核会执行一个函数，该函数叫做中断处理程序(interrupt handler)或中断服务例程(interrupt service routine,ISR)。
<1>产生中断的每个设备都有一个相应的中断处理程序。
	一个设备的中断处理程序是它设备驱动程序的一部分。设备驱动程序是用于对设备进行管理的内核代码。
	
<2>中断处理程序与其他内核函数的真正区别在于，中断处理程序是被内核调用来响应中断的，而它们运行于中断上下文中。
	中断上下文偶尔也称为原子上下文，该上下文中执行的代码不可阻塞。

<3>中断可能随时发生，因此中断处理程序也就随时可能执行。
	中断处理程序要负责通知硬件设备中断已被接收。
	
7.3 上半部与下半部的对比
1、一般把中断处理分为两部分。
<1>上半部(top half) 中断处理程序是上半部，接收到一个中断，它就立即开始执行，但只做有严格时限的工作。
<2>下半部(bottom half) 能够被允许稍后完成的工作会推迟到下半部。此后，在合适的时机，下半部会被开中断执行。

7.4 注册中断处理程序
1、驱动程序可以通过request_irq()函数注册一个中断处理程序(它被声明在linux/interrupt.h中),并且激活给定的中断线，以处理中断。

7.6 中断上下文
1、进程上下文是一种内核所处的操作模式，此时内核代表进程执行，可以通过current宏关联当前进程。

2、中断上下文和进程没有什么瓜葛。与current宏也不想干(尽管它会指向被中断的进程)。
<1>因为没有后备进程，所以中断上下文不可以睡眠，否则不能再对它重新调度。(这是对什么样的函数可以在中断处理程序中使用的限制)
<2>中断上下文具有较为严格的时间限制，因为它打断了其他代码。(甚至是打断了其他中断线上的另一个中断处理程序)
	因此，尽量把工作从中断处理程序中分离出来，放在下半部来执行。
<3>中断处理程序栈的设置是一个配置选项，它们曾经共享所中断进程的内核栈。内核栈的大小是两页，32位系统上是8KB，64位系统上是16KB。

7.7 中断处理机制的实现

第八章 下半部和推后执行的工作
1、中断处理程序存在一些局限：
<1>中断处理程序以异步方式执行，并且它可能会打断其他重要代码(甚至包括其他中断处理程序)的执行。因此，中断处理程序应该执行得越快越好。
<2>如果当一个中断处理程序正在执行，在最好的情况下(没有设置IRQF_DISABLED)，与该中断同级的其他中断会被屏蔽;
	最坏情况下(设置了IRQF_DISABLED)，当前处理器上所有其他中断都会被屏蔽。禁止中断后，硬件与操作系统无法通信。
<3>由于中断处理程序往往需要对硬件进行操作，所以它们通常有很高的时限要求。
<4>中断处理程序不在进程上下文中运行，所以它们不能阻塞。这限制了它们所做的事情。


第九章 内核同步介绍
1、
<1>在使用共享内存的应用程序中，程序员必须特别留意保护共享资源，防止共享资源并发访问。内核也不例外。
<2>并发访问共享数据是造成系统不稳定的一类隐患。

2、从2.0版本开始内核开始支持对称多处理器(SMP)，意味着内核代码可以同时运行在两个或多个处理器上。
	如果不加保护，运行在两个不同处理器上的内核代码完全可能在同一时刻里并发访问共享数据。

3、随着2.6内核的出现，linux内核已经发展为抢占式内核，这意味着，如果不加保护，调度程序可以在任何时刻抢占正在运行的内核代码，重新调度其他的进程执行。

4、执行线程：指任何正在执行的代码实例。比如，一个在内核执行的进程、一个中断处理程序或一个内核线程。

9.1 临界区和竞争条件
<1>所谓临界区(临界段)就是访问和操作共享数据的代码段。
	多个执行线程并发访问同一个资源通常是不安全的，为了避免在临界区中并发访问，必须保证这些代码原子地执行。
	
<2>如果两个执行线程有可能处于同一个临界区中同时执行，那么这就是程序的bug，称为竞争条件。
	避免和防止竞争条件称为同步(sync)。

一、为什么我们需要保护
1、临界区无处不在

二、单个变量
多数处理器都提供了指令来原子地读变量、增加变量，然后再写回变量。
原子操作交错执行根本不可能发生，因为处理器会从物理上确保这种不可能。

9.2 加锁
1、锁由多种多样的形式，而且加锁的粒度范围也各不相同。
2、锁采用原子操作实现，原子操作不存在竞争。

一、造成并发执行的原因
1、内核中造成并发执行的原因
<1>中断
<2>软中断和tasklet
<3>内核抢占
<4>睡眠及用户空间的同步
<5>对称多处理

2、在中断处理程序中能避免并发访问的安全代码称为中断安全代码(interrupt-safe);
   在对称多处理的机器中能避免并发访问的安全代码称为SMP安全代码(SMP-safe);
   在内核抢占时能避免并发访问的安全代码称为抢占安全代码(preempt-safe)。
   
二、了解要保护什么
1、找出哪些数据需要保护
<1>执行线程的局部数据仅仅被它本身访问，显然不需要保护；(局部变量独立存在于执行线程的栈中；动态分配的结构独立存在于堆栈中，它们都不需要任何形式的锁。)
<2>如果数据只会被特定的进程访问，那么不需要加锁。因为进程一次只在一个处理器上执行。

2、大多数内核数据结构都需要加锁
<1>如果其他执行线程可以访问这些数据，那么就给这些数据加上某种形式的锁；
<2>如果任何其他什么东西都能看到它，那么就要锁住它。
<3>要给数据加锁，而不是给代码加锁！！！

3、配置项：SMP与UP
<1>CONFIG_SMP配置选项控制内核是否支持SMP；
	许多加锁问题在单处理器上是不存在的。当CONFIG_SMP没被设置时，不必要的代码就不会被编入针对单处理器的内核映像中。
	这样可以使单处理器机器避免使用自旋锁带来的开销。

<2>同样的技巧也适用于CONFIG_PREEMPT(允许内核抢占的配置选项)。	
   
4、几乎访问所有的内核全局变量和共享数据都需要某种形式的同步方法：
<1>这个数据是不是全局的，除了当前线程外，其他线程能不能访问；
<2>这个数据会不会在进程上下文和中断上下文中共享？它是不是要在两个不同的中断处理程序中共享；
<3>。。。

9.3 死锁
1、死锁产生的条件：
有一个或多个执行线程和一个或多个资源，每个线程都等待其中一个资源，但所有的资源都已经被占用。
所有线程都在互相等待，但它们永远不会释放已经占有的资源。

2、自死锁：
如果一个执行线程试图去获取一个自己已经持有的锁，它将不得不等待锁被释放，但因为它正在忙着等待这个锁，所以自己永远也不会有机会释放锁。
最终结果就是死锁。

3、一些简单的代码规则避免死锁：
<1>按顺序加锁，只要嵌套地使用多个锁，就必须按照相同的顺序去获取它们。
<2>防止发生饥饿。
<3>不要重复请求同一个锁。
<4>设计应力求简单。
   
9.4 争用和扩展性
1、锁的争用(简称争用)：是指当锁正在被占用时，有其他线程试图获得该锁。
   锁处于高度争用状态：是指有多个其他线程在等待获得该锁。
   
2、锁的作用是使程序以串行方式对资源进行访问，所以使用锁无疑会降低系统的性能。
	被高度争用的锁会成为系统的瓶颈，严重降低系统性能。
	
3、扩展性：是对系统可扩展程度的一个量度。
	对于操作系统，在谈及可扩展性时就会和大量进程、大量处理器、大量内存等联系起来。
	
4、加锁粒度用来描述加锁保护的数据规模。


第十章 内核同步方法

不同的体系结构：不同的微处理器架构
eg：SPARC体系结构   支持SPARC的LINUX发行版

10.1 原子操作
1、它是其他同步方法的基石。原子操作可以保证指令以原子的方式执行，执行过程中不被打断。
2、内核提供了两组原子操作接口--一组针对整数进行操作，另一组针对单独的位进行操作。

一、原子整数操作
1、只能对atomic_t类型的数据进行处理。 atomic_t - int

2、atomic_inc(&v) //原子地从v+1
   atomic_dec(&v) //原子地从v-1
   
3、原子操作通常是内联函数，往往是通过内嵌汇编指令来实现的。
	如果某个函数本身就是原子的，那么它往往会被定义成一个宏。
	
4、原子性与顺序性的比较
<1>一个字长的读写总是原子地发生，绝对不可能对同一个字交错地进行读写；
<2>如果要求读必须在待定的写之前发生，这种需求其实不是原子性要求，而是顺序要求。

5、在编写代码时，能使用原子操作时，就尽量不要使用复杂的加锁机制。
	对多数体系结构来讲，原子操作与更复杂的同步方法相比，给系统带来的开销更小，对高速缓存行(cache-line)的影响也小。
   
二、64位原子操作
1、atomic_t变量大小无法在体系结构之间改变，所以atomic_t类型即使在64位体系结构下也是32位的。
32位 atomic_t   
64位 atomic64_t

三、原子位操作   asm/bitops.h
1、参数为一个指针和一个位号。第0位是给定地址的最低有效位，对于32位机器，第31位是给定地址的最高有效位。第32位是下一个字的最低有效位。
	对位号的范围没有0--31的限制。
	
2、原子位操作是对普通的指针进行的操作，所以不像原子整型对应atomic_t，这里没有这个特殊的数据类型。
	只要指针指向了任何你希望的数据，就可以对它进行操作。
   
3、非原子位操作
   
10.2 自旋锁
1、临界区可能跨越多个函数。简单的原子操作对此无能为力，这就需要使用更为复杂的同步方法--锁。
2、linux内核中最常见的锁是自旋锁。
<1>自旋锁最多只能被一个可执行线程持有。如果一个执行线程试图获得一个已被持有的自旋锁，那么该线程就会一直进行忙循环--旋转--等待锁重新可用。
<2>一个被争用的自旋锁使得请求它的线程在等待锁重新可用时自旋(特别浪费处理器时间)，所以自旋锁不应该被长时间持有。
	在现在的抢占式内核中，这点尤为重要。因为锁的持有时间等价于系统的调度等待时间。
<3>使用自旋锁的初衷：段时间内进行轻量级加锁。
   
一、自旋锁的方法
1、自旋锁的实现和体系结构密切相关，代码往往通过汇编实现。
2、自旋锁不可递归：
	如果试图得到一个你正在持有的锁，你必须自旋，等待你自己释放这个锁。但出于自旋忙等的你，永远没有机会释放锁，于是会造成死锁。

3、自旋锁可以使用在中断处理程序中
<1>此处不能使用信号量，因为它们会导致睡眠。
<2>在中断程序中使用自旋锁时，一定要在获取锁之前，首先禁止本地中断(在当前处理器上的中断请求)，
	否则，中断处理程序就会打断正在持有锁的内核代码，有可能会试图去争用这个已经被持有的自旋锁。
	这样一来中断处理程序就会自旋，等待该锁重新可用，但锁的持有者在这个中断处理程序执行完毕前不可能运行。
<3>内核提供的禁止中断同时请求锁的接口：
	spin_lock_irqsave() //保存中断的当前状态，并禁止本地中断，然后在获取指定的锁
	spin_unlock_irqrestore() //对指定的锁解锁，然后让中断恢复到加锁前的状态(当前中断、是否要禁止本地中断)
   
   
10.4 信号量(semaphore)
1、linux中的信号量是一种睡眠锁。如果有一个任务试图获得一个不可用(已经被占用)的信号量时，信号量会将其推进一个等待队列，然后让其睡眠。
	这时处理器能够重获自由，从而去执行其他代码。
	当持有的信号量可用(被释放)后，处于等待队列中的那个任务将被唤醒，并获得该信号量。

2、信号量比自旋锁提供了更好的处理器利用率，因为没有把时间花费在忙等上；但是信号量比自旋锁有更大的开销(进程切换)。

3、
<1>由于争用信号量的进程在等待锁重新变为可用时会睡眠，所以信号量适用于锁会被长时间持有的情况。
<2>相反，锁被短时间持有时，使用信号量就不太适宜了。因为睡眠、维护等待队列以及唤醒所花费的开销可能比锁被占用的全部时间还要长。
<3>由于执行线程在锁被争用时会睡眠，所以只能在进程上下文中才能获取信号量锁，因为在中断上下文中是不能进行调度的。
---------------------------------------------------------------------------------------------------------------------------
1、进程上下文：
	可执行程序代码是进程的重要组成部分。这些代码从一个可执行文件载入到进程的地址空间执行。一般程序在用户空间执行。
当一个程序执行了系统调用或者出发了某个异常，它就陷入了内核空间。此时，我们称内核“代表进程执行”并处于进程上下文中。
除非在此间隙有更高优先级的进程需要执行并由调度器做出了相应调整，否则在内核退出的时候，程序恢复在用户空间会继续执行。
<1>在进程上下文中可以通过current宏关联当前进程。
<2>进程是以进程上下文的形式连接到内核中的，因此，进程上下文可以睡眠，也可以调用调度程序。

2、中断上下文：
	当执行一个中断处理程序时，内核处于中断上下文中。
<1>中断上下文与进程没有什么瓜葛。因为没有后备进程，所以中断上下文不可以睡眠，否则不能对它重新调度。
<2>为了保持同步，内核可以停用中止--既可以停止所有的中断，也可以有选择地停止某个中断号对应的中断。

3、这些上下文代表着内核活动的范围。实际上我们可以将每个处理器在任何指定时间点上的活动必然为以下三者之一：
<1>运行于用户空间，执行用户进程
<2>运行于内核空间，处于进程上下文，代表某个特定进程执行
<3>运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断。
特殊情况，当CPU空闲时，内核就会运行一个空进程，处于进程上下文，但运行于内核空间。
---------------------------------------------------------------------------------------------------------------------------
<4>可以在持有信号量时去睡眠，因为其他进程试图获取一个信号量时，不会因此而死等。
<5>在你占用信号量时不能同时占用自旋锁。

4、
<1>往往在需要和用户空间同步，且代码需要睡眠时，此时使用信号量是唯一的选择。
<2>根据锁被持有的时间长短做判断，来选择自旋锁和信号量。	
<3>信号量不同于自旋锁，它不会禁止内核抢占，这意味着信号量不会对调度的等待时间带来负面影响。  

---------------------------------------------------------------------------------------------------------------------------
信号(signal)：
	当一个进程收到一个信号与处理器收到一个中断请求可以说是一样的。
信号是进程间通信机制中唯一的异步通信机制，一个进程不必通过任何操作来等待信号的到达，事实上，进程也不知道信号什么时候到达。
进程之间可以互相通过系统调用kill发送软中断信号。内核也可以因为内部事件而给进程发送信号，通知进程发生了某个事件。

http://blog.csdn.net/violet_echo_0908/article/details/51201278
信号量：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。
	因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

信号：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。


1、bash：
linux通过信号进行进程间通讯。通过对脚本进行编程，使其在收到linux系统的特定信号时执行特定的命令。
<1>重温linux信号
1     SIGHUP   挂起进程          //shell会退出，退出前将SIGHUP信号传给shell启动的所有进程
2     SIGINT   终止进程          //中断shell，linux内核会停止将CPU处理时间分配给shell，shell会将SIGINT信号传给shell启动的所有进程
3     SIGQUIT  停止进程
9     SIGKILL  无条件终止进程
15    SIGTERM  可能的话终止进程
17    SIGSTOP  无条件停止进程，但不终止
18    SIGTSTP  停止或暂停进程，但不终止
19    SIGCONT  继续运行停止的进程
默认情况下，bash会忽略收到的3和15信号
            bash会处理收到的1和2信号
shell会将这些信号传递给shell脚本来处理，可以在脚本中加入识别信号和处理信号结果的命令。

<2>产生信号
1)终止进程
ctrl+c会产生SIGINT信号
2)暂停进程
ctrl+z会产生SIGTSTP信号 //停止shell中运行的任何进程
停止一个进程跟终止一个进程不同，停止进程会让程序继续保留在内存中，并能从上次停止的位置继续运行
[root@forimg make_test]#sleep 1000
^Z
[1]+  Stopped                 sleep 1000
方括号中的数字是shell分配的作业号，将shell中运行的每个进程称为作业，并为每个作业分配一个唯一的作业号。第一个为1，第二个为2，以此类推。。

2、调试：
Linux下的异常是通过信号的方式来发出的。每个信号都有其对应的名称以及含义。利用man 7 signal 可以查看信号的相关信息。
在linux信号中，最常见的就是下面几种信号，tulip的异常对这几种信号都挂载了信号处理函数来捕捉这几种异常。

#define SIGSEGV         11     段错误异常
SIGSEGV是C/C++程序员很熟悉的信号。当程序没有权利访问一个受保护的地址时，或者访问无效的虚拟内存地址时，会产生这个信号。

#define SIGFPE          8      浮点异常
当进程发生一个浮点错误时，SIGFPE信号被发送给该进程。对于那些处理复杂数学运算的程序，一般会建议你捕获该信号。

#define SIGBUS          7      总线错误
CPU检测到数据总线上的错误时将产生SIGBUS信号。当程序尝试去访问一个没有正确对齐的内存地址时就会产生该信号。

#define SIGABRT         6      异常终止
SIGABRT提供了一种在异常终止(abort)一个进程的同时创建一个核心转储的方法。

#define SIGILL          4      非法指令
如果正在执行的进程中包含非法指令，操作系统将向该进程发送SIGILL信号。如果你的程序使用了线程，或者pointer functions，那么可能的话可以尝试捕获该信号来协助调试。

---------------------------------------------------------------------------------------------------------------------------

10.6 互斥体(muxtex) 
1、信号量用途更通用，没有多少限制。信号量适合于哪些较复杂、未明情况的互斥访问，比如内核与用户空间复杂的交互行为。
2、互斥体--一个更简单的睡眠锁。
	互斥体是一种互斥信号。mutex在内核中对应数据结构mutex，其行为和使用计数为1的信号量类似，但操作接口更简单、实现更高效，而且使用限制更强。
	
3、mutex的简洁和高效性源自于相比使用信号量更多的受限性：
<1>任何时刻中只有一个任务可以持有mutex，也就是说，mutex的使用计数永远是1；
<2>给mutex上锁者必须负责给其再解锁。
	不能在一个上下文中锁定一个mutex，而在另一个上下文中解锁。这个限制使得mutex不适合内核通用户空间复杂的同步场景。
	最常用的场景：在同一个上下文中上锁和解锁。
<3>不允许递归地上锁和解锁。不能递归地持有同一个锁，同样也不能再去解锁一个已被解开的mutex。
<4>当持有一个mutex时，进程不允许退出。
<5>mutex不能在中断或者下半部中使用。
<6>mutex只能通过官方API管理。
当打开内核配置选项CONFIG_DEBUG_MUTEXES后，就会有多种检测来确保这些约束得以遵守。

10.7 完成变量(completion variable) linux/completion.h
1、如果内核中一个任务需要发出信号通知另一个任务发生了某个特定事件，利用完成变量是使两个任务得以同步的简单方法。
2、如果一个任务要执行一些工作时，另一个任务就会在完成变量上等待。当这个任务完成工作后，会使用完成变量去唤醒在等待的任务。
<1>很像信号量。事实上完成变量仅仅提供了代替信号量的一个简单的解决方法。
	例如当子进程执行或者退出时，vfork()系统调用使用完成变量唤醒父进程。
	
10.8 BLK:大内核锁
1、BLK是全局自旋锁。

10.9 顺序锁
1、简称为seq锁。与读写自旋锁类似。


第十二章 内存管理 http://blog.csdn.net/a675311/article/details/49301967
内核本身不能像用户空间那样奢侈地使用内存。内核一般不能睡眠。

12.1 页
1、内核把物理页作为内存管理的基本单位。
	尽管处理器的最小可寻址单位通常是字(甚至字节)，但内存管理单元(MMU，管理内存并把虚拟地址转换为物理地址的硬件) 通常以页为单位进行处理。
	从虚拟内存的角度来看，页就是最小单位。

2、体系结构不同，支持页的大小也不同，某些体系结构支持几种不同的页大小。
	通常32位体系结构--4KB页  //在支持4KB页大小并有1GB物理内存的机器上，物理内存会被划分为262144个页。
	    64位体系结构--8KB页
	
<1>内核用page结构体表示系统中的每个物理页。mm_types.h
<2>一个页可以由页缓存使用；或者作为私有数据；或者作为进程页表中的映射。
<3>page结构与物理页相关，而并非与虚拟页相关。该结构对页的描述只是短暂的。即使页中所包含的数据继续存在，由于交换等原因，它们也可能并不再和同一个page结构相关联。
	内核仅仅用这个数据结构来描述当前时刻在相关的物理页中存放的东西。这种结构的目的在于描述物理内存本身，而不是描述包含在其中的数据。
<4>内核用这一结构来管理系统中所有的页，因为内核需要知道一个页是否空闲。
	如果页已经被分配，内核还需要知道谁拥有这个页。拥有者可能是用户空间进程、动态分配的内核数据、静态内核代码、页高速缓存等。
<5>系统中每个页都要分配这样一个结构。

12.2 区
1、由于硬件的限制，内核并不对所有的页一视同仁。有些页位于内存中特定的物理地址上，所以不能将其用于一些特定的任务。
	由于这种限制，所以内核把页划分为不同的区(zone)。
	
2、linux必须处理如下两种由于硬件缺陷而引起的内存寻址问题：
<1>一些硬件只能用某些特定的内存地址来执行DMA(直接内存访问)
<2>一些体系结构的内存的物理寻址范围比虚拟寻址范围大很多。这样，就有一些内存不能永久地映射到内核空间上。
	ZONE_DMA
	ZONE_DMA32
	ZONE_NORMAL
	ZONE_HIGHMEM //高端内存，其中的页不能永久地映射到内核地址空间。
	
3、区的实际使用和分布是与系统结构相关的。例如，某些体系结构在内存的任何地址上执行DMA都没有问题。

4、linux把系统的页划分为区，形成不同的内存池，这样就可以根据用途进行分配了。
<1>区的划分没有任何物理意义，只不过是内核为了管理页而采取的一种逻辑上的分组。
<2>某些分配可能需要从特定的区中获取页，而另一些分配则可以从多个区中获取页。
	比如，DMA的内存必须从ZONE_DMA中进行分配
		  一般用途的内存既可以从ZONE_NORMAL分配，也可以从ZONE_DMA分配。不过不可能同时从两个区分配，因为分配是不可能跨区界限的。
<3>不是所有的体系结构都定义了全部区。
	比如，inter的x86-64体系结构可以映射和处理64位的内存空间，所以它没有ZONE_HIGHMEM区，所有的物理内存都处于ZONE_DMA和ZONE_NORMAL区。
	
<4>每个区都用zone结构体表示 mmzone.h
	系统中只有三个区，因此，也只有三个这样的结构。
	
10.3 获得页
1、内核提供了一种请求内存的底层机制，并提供了对它进行访问的几个接口(gfp.h)。所有的这些接口都以页为单位分配内存。
	struct page* alloc_page() //分配指定个数连续的物理页，并返回指向第一个页的指针
	void * page_address(struct page * page) //把给定的页转换成它的逻辑地址
<1>调用获取页的接口时，要注意进行错误检查。内核分配可能失败。
	
一、获得填充为0的页  //返回页的内容全部为0
get_zeroed_page() 如果分配的页是给用户空间的，该函数非常有用。
分配好的页中应该包含的都是随机产生的垃圾信息，但其实这些信息可能并不是完全随机的。很可能“随机地”包含某些敏感数据。
用户空间的页在返回之前，所有数据必须填充为0，或做其他清理工作。

二、释放页
1、释放页时要谨慎，只能释放属于自己的页。传递了错误的page或地址，用了错误的order值，都可能导致系统崩溃。
<1>内核是完全信赖程序员的，这点与用户空间不同，如果你有非法的操作，内核会挂起，停止运行。


12.4 kmalloc()  slab.h
1、kmalloc函数与用户空间的malloc一族函数非常类似。用它可以获得以字节为单位的一块内核内存。
<1>除非没有足够的内存可用，否则内核总能分配成功。
<2>要进行返回值错误检查

一、gfp_mask标志
1、不管在低级页分配函数还是在kmalloc中，都用到了分配器标志。
<1>这些标志可分为三类：
	行为修饰符：内核应当如何分配所需的内存。某些特定情况下，只能使用某种特定的方法分配内存。例如，中断处理程序就要求内核在分配内存的过程中不能睡眠。
	区修饰符：从哪个区分配内存。
	类型：组合了行为修饰符合区修饰符，将各种可能用到的组合归纳为不同类型，简化了修饰符的使用。eg：GFP_KERNEL
	
二、kfree() slab.h
1、kmalloc的另一端就是kfree。
<1>kfree释放由kmalloc分配出来的内存块。如果想要释放的内存不是由kmalloc分配的；或者想要释放的内存早就被释放了，
调用这个函数就会导致严重的后果。

12.5 vmalloc
1、vmalloc只确保页在虚拟地址空间内是连续的。它通过分配非连续的物理内存块，再修正页表，把内存映射到逻辑地址空间的连续区域中。
   kmalloc确保页在物理地址上是连续的，虚拟地址自然也是连续的。

2、大多数情况下，只有硬件设备需要得到物理地址连续的内存。在很多体系结构上，硬件设备存在于内存管理单元以外，它根本不理解什么是虚拟地址。
   而仅供软件使用的内存块就可以使用只有虚拟地址连续的内存块。但在编程中，根本察觉不到这种差异。对内核而言，所有内存看起来都是逻辑上连续的。
   
3、但很多内核代码都是使用kmalloc来获取内存的，这主要出于性能的考虑。
<1>vmalloc为了把物理上不连续的页转换为虚拟地址空间上连续的页，必须专门建立页表项。
	通过vmalloc获得的页必须一个一个地进行映射，这就会导致比直接内存映射大得多的TLB抖动。
	TLB(translation lookaside buffer)是一种硬缓冲区，很多体系结构用它来缓存虚拟地址到物理地址的映射关系。
		它极大地提高了系统的性能，因为大多数内存都要进行虚拟寻址。

<2>vmalloc只在不得已时才会使用，典型的就是为了获得大块内存时。
	例如，当模块被动态插入到内核中时，就把模块装载到由vmalloc分配的内存上。
	
12.6 slab层
1、为了便于数据的频繁分配和回收，常常用到空闲链表。
<1>空闲链表包含可供使用的、已经分配好的数据结构块。
<2>当代码需要一个新的数据结构实例时，就可以从空闲链表中抓取一个，而不需要分配内存。
<3>当不再需要这个数据结构实例时，就把它放回空闲链表，而不是释放它。
空闲链表相当于对象高速缓存，快速存储频繁使用的对象类型。

2、linux内核提供了slab层(也就是所谓的slab分配器)，它扮演了通用数据结构缓存层的角色。
slab分配器的几个基本原则
<1>频繁使用的数据结构也会频繁分配和释放，因此应当缓存它们。
<2>频繁分配和回收必然会导致内存碎片(难以找到大块连续的可用内存)。为了避免这种现象，空闲链表的缓存会连续地存放。
	因为已释放的数据机构又会放回空闲链表，因此不会导致碎片。
<3>回收的对象可以立即投入下一次分配，因此，对于频繁的分配和释放，空闲链表能够提高其性能。
<4>如果分配器知道对象大小、页大小和总的高速缓存的大小这样的概念，它会做出更明智的决策。
<5>如果将部分缓存专属于单个处理器，那么，分配和释放就可以在不加SMP锁的情况下进行。
<6>如果分配器是与NUMA相关的，它就可以从相同的内存节点为请求者进行分配。
<7>对存放的对象进行着色，以防止多个对象映射到相同的高速缓存行。

一、slab层的设计
1、slab层把不同的对象划分为所谓高速缓存组，其中每个高速缓存组都存放不同类型的对象。每种对象类型对应一个高速缓存。
	eg:存放进程描述符的高速缓存、存放索引节点的高速缓存。
<1>kmalloc接口建立在slab层之上，使用了一组通用高速缓存。

2、这些高速缓存又被划分为slab，slab由一个或多个物理上连续的页组成。
<1>一般情况下，slab仅仅由一页组成。每个高速缓存可以由多个slab组成。
<2>每个slab都包含一些对象成员，这里的对象是指被缓存的数据结构。
<3>每个slab处于三种状态之一：满、部分满、空
<4>当内核的某一部分需要一个新的对象时，先从部分满的slab中进行分配。如过没有部分满的slab，就从空的slab中分配。
	如过没有空的slab，就要创建一个slab了。 //这种策略用于减少碎片
	
12.7 在栈上的静态分配
1、当给每个进程分配一个固定大小的栈后，不但可以减少内存的消耗，而且内核也无须负担太重的栈管理任务。
<1>内核栈大小既依赖体系结构，也与编译时的选项有关。每个进程都有两页的内核栈。
	32位体系结构页面大小 4KB，内核栈8KB
	64位体系结构页面大小 8KB，内核栈16KB

一、单页内核栈

二、在栈上光明正大地工作(内核栈)
<1>每个进程的整个调用链必须放在自己的内核栈中。
<2>中断处理程序也曾使用它们所中断进程的内核栈。而当我们转而使用只有一个页面的内核栈时，中断处理程序放在中断栈中。
<3>内核栈可以是一页，也可以是两页，这取决于编译时配置选项。栈大小因此在4--16KB的范围内。
<4>在任何情况下，无限制的递归和alloca都是不允许的。//alloca在栈上申请空间
<5>在任意一个函数中，都必须尽量节省栈资源。让所有的局部变量所占空间之和不超过几百字节。

12.8 高端内存的映射
1、高端内存中的页不能永久地映射到内核地址空间上(逻辑地址)。__GFP_HIGHMEM

12.9 每个CPU的分配
1、支持SMP的现代操作系统使用每个CPU上的数据，对于给定的处理器其数据是唯一的。
<1>每个CPU的数据存放在一个数组中。my_percpu[NR_CPUS] 数组中的每一项对应着系统上一个存在的处理器。
<2>除了当前处理器之外，没有其他处理器可以解除这个数据，不存在并发访问问题。

2、所以，内核抢占成为了唯一需要关注的问题
<1>如果你的任务被其他处理器抢占并重新调度，那么这时CPU变量就会无效，因为它指向的是错误的处理器。 //不同CPU间 (当前任务睡眠、被抢占)
<2>如果另一个任务抢占了你的任务，那么有可能在同一个处理器上发生并发访问my_percpu的情况，这属于一个竞争条件。 //同一个CPU内

12.10 新的每个CPU接口
2.6内核为方便创建和操作每个CPU数据，从而引进了新的操作接口，称为percpu。

12.11 使用每个CPU数据的原因
“只有这个处理器能访问这个数据”纯粹是一个编程约定，系统本身并不存在任何措施实现。
1、减少了数据锁。
2、可以大大减少缓存失效。
	缓存失效发生在处理器试图使它们的缓存保持同步时。如果一个处理器操作某个数据，而该数据又存在其他处理器缓存中，
那么存放该数据的那个处理器必须清理或刷新自己的缓存。
	持续不断的缓存失效称为缓存抖动，这样对系统性能影响颇大。
	
3、使用每个CPU数据会省去很多数据上锁，它唯一的安全要求就是要禁止内核抢占。这点代价比上锁要小很多，而且接口会自动帮你完成这个步骤。

4、每个CPU数据在中断上下文和进程上下文中使用都很安全。但要注意，不能在访问每个CPU数据过程中睡眠。
	否则，你就可能醒来后已经到其他处理器上了。

12.12 分配函数的选择


第15章 进程地址空间
1、内核除了管理本身的内存外，还需要管理用户空间中进程的内存。
	我们称这个内存为进程地址空间，也就是系统中每个用户空间进程所看到的内存。

2、linux采用虚拟内存技术，因此，系统中的所有进程之间以虚拟方式共享内存。
<1>对一个进程而言，它好像都可以访问整个系统的所有物理内存。
<2>更重要的是即使单独一个进程，它拥有的地址空间也可以远大于系统物理内存。

15.1 地址空间
1、进程地址空间是由进程可寻址的虚拟内存组成。内核允许进程使用这种虚拟内存中的地址。
<1>每个进程都有一个32位或64位的平坦(flat)地址空间，空间大小取决于体系结构。
<2>平坦是指地址空间范围是一个独立的连续区间。
<3>通常情况下，每个进程都有唯一的这种平坦地址空间。
	两个进程的地址空间即使有相同的内存地址，实际也彼此互不相干。我们称这样的进程为线程。
<4>尽管一个进程可以寻址4GB的虚拟内存(32位地址空间中)，但这并不代表它就有权访问所有的虚拟地址。
	这些可以被进程访问的合法地址空间称为内存区域(memory areas)。
	通过内核，进程可以给自己的地址空间动态地增加或减少内存区域。
	
2、进程只能访问有效内存区域的内存地址。
<1>每个内存区域具有相关权限，如对相关进程有读、写、执行属性。
	如果一个进程访问了不在有效范围中的内存区域，或以不正确的方式访问了有效地址，那么内核就会终止该进程，并返回“断错误”信息。
	
<2>内存区域可以包含各种内存对象：
1)可执行文件代码的内存映射，称为代码段。
2)可执行文件的已初始化全局变量的内存映射，称为数据段。
3)包含未初始化全局变量，也就是bss段的零页的内存映射。
	注：BSS(block started by symbol)：因为未初始化的变量没有对应的值，所以并不需要存放在可执行对象中。
		但是由于C标准强制规定未初始化的全局变量要被赋予特殊的默认值(基本上是0)，所以内核要将未赋值的变量从可执行代码载入到内存中，
		然后将零页映射到该片内存上，于是这些未被初始化变量就被赋予了0值。
		这样做避免了在目标文件中显式地进行初始化，减少了空间浪费。
		零页：页面中的信息全部为0值，所以可用于映射bss段等目的。
4)用于进程用户空间栈的零页的内存映射。
	注：不要和进程内核栈混淆，进程的内核栈独立存在并由内核维护。
5)每一个诸如C库或动态链接程序等共享库的代码段、数据段和bss也会被载入进程的地址空间。
6)任何内存映射文件。
	先说结论：使用内存映射文件来处理大文件可以提高效率。 
为什么呢？
我们先来看看如果不使用内存映射文件的处理流程是怎样的，首先我们得先读出磁盘文件的内容到内存中，然后修改，最后回写到磁盘上。
第一步读磁盘文件是要经过一次系统调用的，它首先将文件内容从磁盘拷贝到内核空间的一个缓冲区，然后再将这些数据拷贝到用户空间，
实际上是两次数据拷贝。第三步回写也一样也要经过两次数据拷贝。
所以我们基本上会有四次数据的拷贝了，因为大文件数据量很大，几十GB甚至更大，所以拷贝的开销是非常大的。

而内存映射文件是操作系统的提供的一种机制，可以减少这种不必要的数据拷贝，从而提高效率。
它由mmap()将文件直接映射到用户空间，mmap()并没有进行数据拷贝，真正的数据拷贝是在缺页中断处理时进行的，
由于mmap()将文件直接映射到用户空间，所以中断处理函数根据这个映射关系，直接将文件从硬盘拷贝到用户空间，
所以只进行了一次数据拷贝 ，比read进行两次数据拷贝要好上一倍，因此，内存映射的效率要比read/write效率高。

一般来说，read write操作可以满足大多数文件操作的要求，但是对于某些特殊应用领域所需要的几十GB甚至更大的存储，
这种通常的文件处理方法进行处理显然是行不通的。目前，对于上述大文件的操作一般是以内存映射文件的方式来加以处理的。

7)任何共享内存段。
8)任何匿名的内存映射，比如由malloc()分配的内存。

<3>进程地址空间中的任何有效地址都只能位于唯一的区域，这些内存区域不能相互覆盖。
	在执行的进程中，每个不同的内存片段都对应一个独立的内存区域：栈、对象代码、全局变量、被映射的文件等。


15.2 内存描述符 
1、内核使用内存描述符结构体(mm_struct linux/sched.h)表示进程的地址空间，该结构体包含了和进程地址空间有关的全部信息。
<1>mm_users：正在使用该地址的进程数目。比如，两个线程共享该地址空间，那么mm_users的值等于2。
<2>mm_count：是mm_struct结构体的主引用计数。当所有使用该地址空间的线程都退出时，mm_count的值才变为0。
	mm_count的值等于0，说明已经没有任何指向该mm_struct结构体的引用了，这时该结构体会被撤销。
	
<3>mmap和mm_rb：这两个不同数据结构体描述的对象是相同的，即为该地址空间中的全部内存区域。
	前者以链表形式存放而后者以红黑树形式存放。

<4>所有的mm_struct结构体都通过自身的mmlist链接在一个双向链表中。
	该链表的首元素是init_mm内存描述符，它代表init进程的地址空间。
	操作该链表时，需要使用mmlist_lock锁来防止并发访问。kernel/fork.c
	
一、分配内存描述符
1、进程描述符 task_struct中有一项：struct mm_struct *mm, *active_mm;
	所以current->mm便指向当前进程的内存描述符。
	
<1>fork函数利用copy_mm函数复制父进程的内存描述符给其子进程。
	而子进程中的mm_struct结构体实际是通过文件kernel/fork.c中的allocate_mm宏从mm_cachep slab缓存中分配得到的。
<2>通常每个进程都有唯一的进程地址空间，如果父进程希望和其子进程共享地址空间，可以在调用clone时，
	设置CLONE_VM标识。把这样的子进程称为线程。CLONE_VM被指定后，内核就不再需要调用allocate_mm()函数了。
	是否共享地址空间几乎是进程和linux中所谓线程之间的唯一区别。
	
二、撤销内存描述符
1、当进程退出时，内核会调用kernel/exit.c中的exit_mm函数，该函数执行一些常规的撤销工作，同时更新一些统计量。
<1>该函数会调用mmput减少内存描述符中的mm_users用户计数，如果用户计数变为0，将调用mmdrop函数，减少mm_count使用计数。
<2>如果使用计数也等于零，说明内存描述符不再有任何使用者了，那么调用free_mm宏通过kmem_cache_free函数将mm_struct结构归还到mm_cachep slab缓存中。

三、mm_struct与内核线程
1、内核进程没有进程地址空间，也没有相关的内存描述符。
	所以内核线程对应的进程描述符中mm为空。这也正是内核线程的真实含义：它们没有用户上下文。
	
2、内核并不需要访问任何用户空间的内存，而且因为内核线程在用户空间中没有任何页，所以实际上它们并不需要有自己的内存描述符和页表。
	尽管如此，即使是访问内核内存，内核线程还是需要使用一些数据的，比如页表。
	为了避免内核线程为内存描述符和页表浪费内存，也为了当新内核线程运行时，避免浪费处理器周期向新地址空间进行切换，
		内核线程将直接使用前一个进程的内存描述符。
		
3、当一个进程被调度时，该进程的mm指向的地址空间被装载到内存，进程描述符中的active_mm会被更新，指向新的地址空间。
<1>内核线程没有地址空间，mm为NULL，当一个内核线程被调度时，就会保留前一个进程的地址空间。
	随后内核更新内核线程对应的进程描述符中的active_mm，使其指向前一个进程的内存描述符。
	所以在需要时，内核线程便可以使用前一个进程的页表。
	
<2>因为内核线程不访问用户空间的内存，仅仅使用地址空间中的内核内存相关的信息，这些信息的含义和普通进程完全相同。

15.3 虚拟内存区域
1、内存区域由vm_area_struct描述，定义在linux/mm_type.h中。内存区域在linux内核中也称为虚拟内存区域(VMAs)。
vm_area_struct是内存描述符mm_struct结构中的一项：
struct vm_area_struct *mmap; //内存区域链表
struct vm_area_struct *mmap_cache; //最近使用的内存区域

<1>vm_area_struct描述了指定地址空间内连续区间上的一个独立内存范围。
内核将每个内存区域作为一个单独的内存对象管理，每个内存区域都拥有一致的属性，比如访问权限等，另外，相应的操作也都一致。
每一个VMA可以代表不同类型的内存区域，比如内存映射文件或者进程用户空间栈。

<2>每个内存区域都对应进程地址空间中的唯一区间。[vm_start,vm_end)
1)vm_start指向区间的首地址(最低地址)；
2)vm_end指向区间的尾地址(最高地址)之后的第一个字节。
3)vm_mm指向和VMA相关的mm_struct结构体。
每个VMA对其相关的mm_struct结构体来说都是唯一的。所以即使两个独立的进程将同一个文件映射到各自的地址空间，
	它们分别都会有一个VMA来标志自己的内存区域。但如果两个线程共享一个地址空间，那么它们也同时共享其中所有的VMA结构体。

<3>同一个地址空间内的不同VMA之间不能重叠。

一、VMA标志
1、VMA标志是一种位标志，在linux/mm.h中。它包含在vm_flags域内，标志了VMA所包含全部页面的整体行为和信息。
<1>和物理页的访问权限不同，VMA标志反映了内核处理页面所需要遵守的行为规则，而不是硬件要求。

二、VMA操作
1、VMA结构中的vm_ops指向与指定内存区域相关的操作函数表。内核使用表中的方法操作VMA。

三、内存区域的树型结构和内存区域的链表结构
1、可以通过内存描述符中的mmap和mm_rb之一来访问内存区域。
<1>这两个域各自独立指向与内存描述符相关的全体内存区域对象。它们包含完全相同的VMAs结构体指针，仅仅组织方法不同。
<2>mmap使用单独链表连接所有VMAs，每个VMA通过自身的vm_next被连入链表。mmap指向链表中的第一个VMA，链表中的最后一个结构体指针指向NULL。
<3>红黑树的搜索、插入、删除等操作的复杂度都是O(log(n))。
<4>链表用于需要遍历全部节点时，红黑树用于在地址空间中定位特定VMA的时候。

四、实际使用中的内存区域
1、可以在/proc文件系统 或者 使用pmap工具查看给定进程的内存空间和其中所含的内存区域。

2、eg: cat /proc/<pid>/maps  
<1>列出了该进程地址空间中包含的内存区域。包括 代码段、数据段、bss段、进程栈 等。
<2>如果该进程链接了C动态库，那么还会列出动态库对应的 代码段、数据段、bss段。

每行数据格式：
开始-结束     访问权限       偏移量   主设备号：次设备号    设备i节点号   文件
虚拟地址    p-私有 s-共享         

-没有映射文件的内存区域的设备标志为00:00；索引节点标志也为0。这个区域就是零页。(零页映射的内容全部为0)  
	如果将零页映射到可写的内存区域，那么该区域将全被初始化为0。

<3>pmap <pid>输出内容与cat /proc/<pid>/maps一致，但根据有可读性。

<4>代码段具有可读且可执行权限
   数据段和bss具有可读写权限
   堆heap栈stack 具有可读写 甚至可执行的权限
   
1)eg中该进程的全部地址空间大约是1340KB，但只有大约40KB的内存区域是可写和私有的。
如果一片内存范围是共享的或不可写的，那么内核只需要在内存中为文件(backing file)保留一份映射。
映射只用来读，只把映射项读入一次是很安全的。
所以C库在物理内存中仅仅需要占用1212KB空间，而不是为每个使用C库的进程在内存中都保存一个1212KB的空间。

2)由于内存未被共享，所以只要一有进程写该处数据，那么该处数据就将被拷贝出来(写时拷贝，copy-on-write,COW)，然后才被更新。
eg：string str1 = "hello world";
    string str2 = str1;
//str1和str2存放数据的地址是一样的。	
	str1[1] = 'q';
	str2[1] = 'w'
//str1的地址发生了变化，str2的地址还是原来的。

这就是C++中COW技术的应的，不过VS2005似乎不再支持COW了。	


15.4 操作内存区域  linux/mm.h
一、find_vma()


15.7 页表
1、虽然应用程序操作的对象是映射到物理内存之上的虚拟内存，但是处理器直接操作的却是物理内存。
<1>当应用程序访问一个虚拟地址时，首先必须将虚拟地址转化成物理地址，然后处理器才能解析地址访问请求。
<2>地址的转换工作需要通过查询页表才能完成。
地址转化需要将虚拟地址分段，使每段虚拟地址都作为一个索引指向页表，而页表则指向下一级别的页表或指向最终的物理页面。

2、linux使用三级页表完成地址转化。利用多级页表能够节约地址转换所需占用的存放空间。
<1>顶级页表是页全局目录(PGD)
<2>二级页表是中间页目录(PMD)
<3>最后一级页表简称页表(PTE)，该页表指向物理页面。

3、多数体系结构中，搜索页表的工作是由硬件完成的，但只有在内核正确设置页表的前提下，硬件才能方便地操作它们。

4、每个进程都有自己的页表(线程会共享页表)。
<1>内存描述符mm的pgd域指向的就是进程的页全局目录。
<2>操作和检索页表时，必须使用page_table_lock锁。该锁在相应的进程描述符中。
<3>页表对应的结构体依赖与具体的体系结构。定义在asm/page.h中。

5、页表操作的性能非常关键。
<1>为了加快搜索，多数体系结构都实现了一个翻译后缓冲区/转译查找缓存(translate lookaside buffer,TLB)。
1)TLB作为一个将虚拟地址映射到物理地址的硬件缓存，当请求访问一个虚拟地址时，处理器将首先检查TLB中是否缓存了该虚拟地址到物理地址的映射。
如果在缓存中直接命中，物理地址立刻返回；否则，就需要再通过页表搜索需要的物理地址。

6、页表管理的改进
<1>2.6内核对页表管理的改进是：从高端内存分配部分页表
<2>今后的改进：通过写时拷贝(COW)的方式共享页表
1)这种机制使得在fork操作中可由父子进程共享页表。
只有当子进程或父进程试图修改特定页表项时，内核才去创建该页表项的新拷贝，此后父子进程才不再共享该页表项。
利用共享页表可以消除fork操作中页表拷贝所带来的消耗。


第16章 页高速缓存和页回写
1、页高速缓存(cache)是linux内核实现磁盘缓存。它主要用来减少对磁盘的I/O操作。
<1>通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。
1)访问磁盘的速度远远低于访问内存的速度。ms和ns的差距。
2)数据一旦被访问，就很有可能在短期内再次被访问。

<2>临时局部原理(temporal locality)：如果在第一次访问数据时缓存它，那么极有可能在短期内再次被高速缓存命中。

<2>页回写：将页高速缓存中的变更数据刷新回磁盘的操作。

16.1 缓存手段
1、页高速缓存是由内存中的物理页面组成的，其内容对应磁盘上的物理块。
<1>页高速缓存能动态调整。它可以占用空闲内存来扩张，也可以自我收缩来缓解内存使用的压力。
<2>正被缓存的存储设备被称为后备存储，因为缓存背后的磁盘才是所有数据的归属。

2、当内核开始一个读操作，它首先会检查需要的数据是否在页高速缓存中。如果在，则放弃访问磁盘，直接从内存中读取。这个行为称为缓存命中。
如果数据没有在缓存中，则称为缓存未命中，那么内核必须调度块I/O操作从磁盘去读取数据。然后内核将读来的数据放入页缓存中。
系统并不一定要将整个文件都缓存，缓存可以持有某个文件的全部内容，也可以存储另一些文件的一页或几页。取决于被访问到的数据。

一、写缓存
1、缓存一般被实现成以下三种策略：
<1>不缓存(nowrite):高速缓存不去缓存任何写操作。当对一个缓存中的数据片进行写时，将直接跳过缓存，写到磁盘。
同时也使缓存中的数据失效，后续读操作进行时需要重新到磁盘中读取数据。--很少使用

<2>写透缓存(write-through cache)：写操作将自动更新内存缓存，同时也更新磁盘文件。
写操作会立刻穿透缓存到磁盘中。这种策略对保持缓存一致性很有好处。
缓存数据时刻与后备存储保持同步，所以不需要让缓存失效。同时实现也最简单。

<3>回写缓存(write-back)：程序执行写操作直接写到缓存中，后备存储不会立刻更新，而是将页高速缓存中被写入的页面标记为脏，
并且被加入到脏页链表中。然后由回写进程周期行将脏页链表中的页写回到磁盘，从而让磁盘中的数据和内存中的最终一致。
1)回写策略正是linux所采用的策略。
2)回写策略通常被认为要好于写透策略，因为通过延迟写磁盘，方便以后的时间内合并更多的数据和再一次刷新。当然，其代价是实现复杂度高了很多。

二、缓存回收
1、缓存算法最后涉及的重要内容是缓存中的数据如何清除。这个工作也就是决定缓存中什么内容将被清除的策略。称为缓存回收策略。
<1>为更重要的缓存项腾出位置
<2>收缩缓存大小，腾出内存

2、linux的缓存回收是通过选择干净页进行简单替换。
<1>如果缓存中没有足够的干净页，内核将强制地进行回写操作，以腾出更多的干净可用页。
<2>理想的回收策略(预测算法)应该是回收那些以后最不可能使用的页面。

3、最近最少使用
最通用的算法(特别对于通用目的的页高速缓存)是 最近最少使用算法(LRU)。
该策略的良好效果源自于缓存的数据越久未被访问，则越不大可能近期再被访问。而最近被访问的最有可能被再次访问。

4、双链策略
linux实现的是一个修改过的LRU，也称为双链策略。
<1>linux维护的不再是一个LRU链表，而是维护两个链表：活跃链表和非活跃链表。
处于活跃链表上的页面被认为是"热"的且不会被换出；而在非活跃链表上的页面则是可以被换出的。

<2>链表从尾部加入，从头部移除，如同队列。
两个链表需要保持平衡，如果活跃链表变得过多而超过了非活跃链表，那么活跃链表的头页面将被重新移回到非活跃链表中，以便能再被回收。

<3>缓存能极大地提高系统性能。为了看到差别，对比一下缓存冷时(cache cold)(也就是说重启后，编译大软件工程的时间)和缓存热时(cache warm)的差别。


16.2 linux页高速缓存



第13章 虚拟文件系统
1、虚拟文件系统有时也称为虚拟文件交换(VFS) 作为内核子系统，为用户空间程序提供了文件和文件系统相关的接口。
<1>系统中所有的文件系统不但依赖VFS共存，而且也依靠VFS系统协同工作。
<2>通过虚拟文件系统，程序可以利用标准的Unix系统调用对不同文件系统，甚至不同介质上的文件系统进行读写操作。

13.1 通用文件系统接口
1、VFS使得用户可以直接使用open\read\write这样的系统调用而无需考虑具体文件系统和实际物理介质。
2、VFS使得用户可以在这些不同的文件系统和介质之间执行--使用标准的系统调用从一个文件系统拷贝数据到另一个文件系统。

13.2 文件系统抽象层
1、之所以可以使用这种通用接口对所有类型的文件系统进行操作，是因为内核在它的底层文件系统接口上建立了一个抽象层。
	为了支持多文件系统，VFS提供了一个通用文件系统模型，该模型囊括了任何文件系统的常用功能集和行为。

2、VFS抽象层之所以能衔接各种各样的文件系统，是因为它定义了所有文件系统都支持的、基本的、概念上的接口和数据结构。










	
	
	
	
	
	
	
	
	
	












