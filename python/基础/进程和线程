一、多进程(multiprocessing)
1、Unix/Linux提供了fork()系统调用。
<1>fork()调用一次，返回两次。操作系统自动把当前进程(父进程)复制了一份(子进程)，然后分别在父进程和子进程内返回。
子进程中永远返回0，父进程中返回子进程的ID。
原因是：一个父进程可以fork出很多子进程，因此，父进程要记录下子进程的ID；而子进程只需要调用getppid()就可以得到父进程的ID。

<2>os模块封装了常见的系统调用，其中就包括fork。
print("Process %d start!" % os.getpid())
pid = os.fork()
if 0 == pid:
	print("I'm child process %d and my parent is %d." % (os.getpid(), os.getppid()))
else:
	print("I'm parent process %d and my child is %d." % (os.getpid(), pid));
	
<3>windows没有fork()系统调用；Mac下可以执行fork()。

<4>有了fork()调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务。
常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork子进程来处理新的http请求。

2、multiprocessing模块   //跨平台版本的多进程模块
multiprocessing模块提供了一个Process类来表示一个进程对象。

from multiprocessing import Process
import os

def run_proc(name):
    print('Run child process %s (%d)...' % (name, os.getpid()))

def main():
    print('Parent process %d.' % os.getpid())
    p = Process(target=run_proc,args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')

Process创建子进程时，需要传入一个执行函数和函数的参数。
process.start()方法启动子进程。
process.join()方法可以等待子进程结束后再继续往下执行(通常用于进程间的同步)。

3、Pool
如果要启动大量的子进程，可以用进程池的方式批量创建子进程。

<1>示例
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%d)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))


def main():
    print('Parent process %d.' % os.getpid())
    p = Pool(4)
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))

    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
	
执行结果：
[root@localhost test]# python multiprocess3.py 
Parent process 16959.
Waiting for all subprocesses done...
Run task 0 (16960)...
Run task 1 (16961)...
Run task 2 (16962)...
Run task 3 (16963)...
Task 3 runs 0.18 seconds.
Run task 4 (16963)...
Task 4 runs 0.18 seconds.
Task 1 runs 1.53 seconds.
Task 0 runs 1.60 seconds.
Task 2 runs 2.21 seconds.
All subprocesses done.

<2>代码解读
1)Pool.join()等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的process了。
2)task 0,1,2,3是立刻执行的，task 4则要等待前面某个task完成后才能执行。
这是因为Pool的大小设为了4，因此最多同时执行4个进程。
(如果不设置Pool大小，即p = Pool()，Pool的默认大小是CPU的核心数。)

4、子进程
很多时候，子进程并不是本身，而是一个外部进程。创建子进程后，还需要控制子进程的输入和输出。
subprocess模块可以非常方便地启动一个子进程，然后控制其输入和输出。

<1>示例
import subprocess

def main():
    print('$ ping 10.96.32.57 -c 4')
    r = subprocess.call(['ping', '10.96.32.57', '-c' , '4'])
    print('Exit code:', r)

执行结果：
[root@localhost test]# python multiprocess4.py     
$ ping 10.96.32.57 -c 4
PING 10.96.32.57 (10.96.32.57) 56(84) bytes of data.
64 bytes from 10.96.32.57: icmp_seq=1 ttl=59 time=1.30 ms
64 bytes from 10.96.32.57: icmp_seq=2 ttl=59 time=0.368 ms
64 bytes from 10.96.32.57: icmp_seq=3 ttl=59 time=0.392 ms
64 bytes from 10.96.32.57: icmp_seq=4 ttl=59 time=0.344 ms

--- 10.96.32.57 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3001ms
rtt min/avg/max/mdev = 0.344/0.602/1.307/0.408 ms
('Exit code:', 0)

6、进程间通讯
操作系统提供了很多机制实现进程间的通信，multiprocessing模块封装了底层的机制，提供了Queue、Pipes等多种方式交换数据。

以Queue为例：

from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()

执行结果：
Process to write: 50563
Put A to queue...
Process to read: 50564
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.


二、多线程
lock.acquire()
try:
    change_it(n)
finally:
   lock.release()
获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try...finally来确保锁一定会被释放。


三、ThreadLocal
1、在多线程环境下，每个线程都有自己的数据。
<1>一个线程使用自己的局部变量比使用全局变量要好，因为局部变量只有线程自己能访问，不会影响其他线程;而全局变量的修改必须加锁。
<2>但局部变量也有问题，就是在函数调用时传递起来很麻烦。
因此出现了ThreadLocal。

2、一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。
ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。

四、进程vs线程
1、多进程模式最大的优点就是稳定性高;但创建进程的代价很大。
<1>UNIX系统下fork调用还行，在windows下创建进程开销很大。
<2>操作系统的进程数是有限的。

2、多线程模式不够稳定。

3、计算密集型和IO密集型
<1>要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。
<2>计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。
Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。
对于计算密集型任务，最好用C语言编写。

<3>涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成。
对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。

IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。
对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。

4、异步IO
<1>如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型。
Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。

<2>在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。
由于系统总的进程数量十分有限，因此操作系统调度非常高效。
用异步IO编程模型来实现多任务是一个主要的趋势。

<3>对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。


四、分布式进程
1、在thread和process之间，应优先选择process，因为它更稳定，且可以分布到多台机器上。thread只能分布到同一台机器的多个CPU上。
<1>multiprocessing模块不但支持多进程，其中managers子模块还支持把多个进程分布到多台机器上。



























